{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ab3aaac-20fc-44ae-8506-99c7ff9ab832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import transforms\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch import Unet\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "from segmentation_models_pytorch.utils import train as smp_train\n",
    "from segmentation_models_pytorch import utils\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.profilers import AdvancedProfiler, SimpleProfiler\n",
    "\n",
    "from torchmetrics.segmentation import MeanIoU, GeneralizedDiceScore\n",
    "from torchmetrics import Accuracy, F1Score, Precision, Recall, ConfusionMatrix, AUROC\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f72cf0ca-c643-4890-8e4e-39bc9680b44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfcc3e4-88f7-435b-b269-7d06a4827b4f",
   "metadata": {},
   "source": [
    "# SET UP INPUT/OUTPUT PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "860e05fc-06ec-40e4-9794-5a3786a9884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evr_dir = '/media/ubuntu/E/EVR_region_files'\n",
    "\n",
    "img_dir = '/media/ubuntu/E/ML_data/imgs'\n",
    "mask_dir = '/media/ubuntu/E/ML_data/masks'\n",
    "binary_mask_dir = '/media/ubuntu/E/ML_data/binary_masks'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea6d1e3-55d0-438b-905a-34b1d46f69ec",
   "metadata": {},
   "source": [
    "# COLLECT ZAR PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e24cdd8b-a5f5-4779-a654-0315082c798b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>zarr</th>\n",
       "      <th>evr_files</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0704</td>\n",
       "      <td>/media/ubuntu/E/processed/040707/1007S-D200707...</td>\n",
       "      <td>[04July.EVR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0704</td>\n",
       "      <td>/media/ubuntu/E/processed/040707/1007S-D200707...</td>\n",
       "      <td>[04July.EVR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0704</td>\n",
       "      <td>/media/ubuntu/E/processed/040707/1007S-D200707...</td>\n",
       "      <td>[04July.EVR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0704</td>\n",
       "      <td>/media/ubuntu/E/processed/040707/1007S-D200707...</td>\n",
       "      <td>[04July.EVR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0704</td>\n",
       "      <td>/media/ubuntu/E/processed/040707/1007S-D200707...</td>\n",
       "      <td>[04July.EVR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>0708</td>\n",
       "      <td>/media/ubuntu/E/processed/080707/1007S-D200707...</td>\n",
       "      <td>[08July.EVR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2708</th>\n",
       "      <td>0708</td>\n",
       "      <td>/media/ubuntu/E/processed/080707/1007S-D200707...</td>\n",
       "      <td>[08July.EVR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2709</th>\n",
       "      <td>0708</td>\n",
       "      <td>/media/ubuntu/E/processed/080707/1007S-D200707...</td>\n",
       "      <td>[08July.EVR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2710</th>\n",
       "      <td>0708</td>\n",
       "      <td>/media/ubuntu/E/processed/080707/1007S-D200707...</td>\n",
       "      <td>[08July.EVR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2711</th>\n",
       "      <td>0708</td>\n",
       "      <td>/media/ubuntu/E/processed/080707/1007S-D200707...</td>\n",
       "      <td>[08July.EVR]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2712 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      date                                               zarr     evr_files\n",
       "0     0704  /media/ubuntu/E/processed/040707/1007S-D200707...  [04July.EVR]\n",
       "1     0704  /media/ubuntu/E/processed/040707/1007S-D200707...  [04July.EVR]\n",
       "2     0704  /media/ubuntu/E/processed/040707/1007S-D200707...  [04July.EVR]\n",
       "3     0704  /media/ubuntu/E/processed/040707/1007S-D200707...  [04July.EVR]\n",
       "4     0704  /media/ubuntu/E/processed/040707/1007S-D200707...  [04July.EVR]\n",
       "...    ...                                                ...           ...\n",
       "2707  0708  /media/ubuntu/E/processed/080707/1007S-D200707...  [08July.EVR]\n",
       "2708  0708  /media/ubuntu/E/processed/080707/1007S-D200707...  [08July.EVR]\n",
       "2709  0708  /media/ubuntu/E/processed/080707/1007S-D200707...  [08July.EVR]\n",
       "2710  0708  /media/ubuntu/E/processed/080707/1007S-D200707...  [08July.EVR]\n",
       "2711  0708  /media/ubuntu/E/processed/080707/1007S-D200707...  [08July.EVR]\n",
       "\n",
       "[2712 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find EVR for each ZARR \n",
    "zarr_dir = '/media/ubuntu/E/processed'\n",
    "zarr_evr_df = pd.DataFrame(columns = ['date', 'zarr', 'evr_files'])\n",
    "counter = 0\n",
    "for zarr_subdir in os.listdir(zarr_dir):\n",
    "    for zarr_file in os.listdir(os.path.join(zarr_dir, zarr_subdir)):\n",
    "        _, date, _ = zarr_file.split('-')\n",
    "        date = date[5:]\n",
    "        month = date[:2]\n",
    "        day = date[2:4]\n",
    "        if month == '06': month = 'June'\n",
    "        elif month == '07': month = 'July'\n",
    "        else: \n",
    "            print(date, 'Different month', month)\n",
    "        evr_date = day + month\n",
    "        evr_files = []\n",
    "        for evr_file in os.listdir(evr_dir):\n",
    "            if evr_file.startswith(evr_date):\n",
    "                evr_files.append(evr_file)\n",
    "    \n",
    "        zarr_file = os.path.join(zarr_dir, zarr_subdir, zarr_file, zarr_file + '_Sv.zarr')\n",
    "        zarr_evr_df.loc[counter] = [date, zarr_file, evr_files]\n",
    "        counter += 1\n",
    "\n",
    "zarr_evr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4627c62-9075-4cdc-b994-b3ce4733bcf2",
   "metadata": {},
   "source": [
    "# CUT ALL DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7335ac5-6a9a-4384-82cc-276f1ceef23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "import echoregions as er\n",
    "import math\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Returning No Mask. Empty 3D Mask cannot be converted to 2D Mask.\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"No gridpoint belongs to any region.\")\n",
    "\n",
    "chunk_sizes = {\n",
    "    'channel': -1,           # Load all channels in one chunk\n",
    "    'ping_time': 100,        # Chunk by 100\n",
    "    'range_sample': 650      # Split into 2 chunks\n",
    "}\n",
    "\n",
    "def find_nan_depths(channel_data):\n",
    "    nan_mask = channel_data.isnull()\n",
    "    all_nans = nan_mask.all(dim='ping_time')\n",
    "    all_nan_depths = channel_data.range_sample.where(all_nans, drop=True).values\n",
    "    return all_nan_depths\n",
    "\n",
    "# Use Dask\n",
    "def load_zarr_lazy(zarr_path, chunk_sizes=chunk_sizes, ignore_vars = []):\n",
    "    return xr.open_zarr(zarr_path, chunks=chunk_sizes, drop_variables = ignore_vars)   \n",
    "\n",
    "def correct_echo_range(ds):\n",
    "    # Replace channel and ping_time with their first elements\n",
    "    first_channel = ds[\"channel\"].values[0]\n",
    "    first_ping_time = ds[\"ping_time\"].values[0]\n",
    "    \n",
    "    # Slice the echo_range to get the desired range of values\n",
    "    selected_echo_range = ds[\"echo_range\"].sel(channel=first_channel, ping_time=first_ping_time)\n",
    "    selected_echo_range = selected_echo_range.values.tolist()\n",
    "    selected_echo_range = [value + 8.6 for value in selected_echo_range]\n",
    "\n",
    "    # Find min and max ignoring NaNs\n",
    "    min_val = np.nanmin(selected_echo_range)\n",
    "    max_val = np.nanmax(selected_echo_range)\n",
    "    \n",
    "    # Assign the values to the depth coordinate, transducer offset 8.6m\n",
    "    ds = ds.assign_coords(range_sample=selected_echo_range)\n",
    "\n",
    "    # Remove nan values\n",
    "    ds = ds.sel(range_sample=slice(min_val, max_val))\n",
    "    \n",
    "    return ds\n",
    "\n",
    "def normalize_each_channel(data):\n",
    "    # Replace NaNs with the minimum value - 10 for each channel\n",
    "    min_vals = np.nanmin(data, axis=(0, 1), keepdims=True)\n",
    "    data = np.where(np.isnan(data), min_vals - 10, data)\n",
    "\n",
    "    # Calculate the minimum value for each channel\n",
    "    min_vals = np.nanmin(data, axis=(0, 1), keepdims=True)\n",
    "    max_vals = np.nanmax(data, axis=(0, 1), keepdims=True)\n",
    "    \n",
    "    # Calculate normalization parameters\n",
    "    ranges = max_vals - min_vals\n",
    "    ranges[ranges == 0] = 1\n",
    "    \n",
    "    # Normalize the data\n",
    "    data_normalized = (data - min_vals) / ranges\n",
    "    return data_normalized\n",
    "\n",
    "def fill_na_with_interpolation(chunk):\n",
    "    for channel in chunk.channel:\n",
    "        chunk.loc[dict(channel=channel)] = chunk.sel(channel=channel).interpolate_na(dim='ping_time', method='linear')\n",
    "    return chunk\n",
    "\n",
    "def load_combine_process_zarrs(zarr_paths, ignore_vars = []):\n",
    "    datasets = [load_zarr_lazy(path, chunk_sizes = {}, ignore_vars = ignore_vars) for path in zarr_paths]\n",
    "    combined_dataset = xr.concat(datasets, dim='ping_time')\n",
    "    combined_dataset = combined_dataset.sortby('ping_time')\n",
    "    # select first 3 channels\n",
    "    combined_dataset = combined_dataset.isel(channel=slice(0, 3))\n",
    "    # remove empty pings\n",
    "    combined_dataset = combined_dataset.dropna(dim='ping_time', how='all', subset=['Sv'])\n",
    "    combined_dataset = correct_echo_range(combined_dataset)\n",
    "    #combined_dataset = apply_remove_background_noise(combined_dataset)\n",
    "    combined_dataset = combined_dataset.rename({'range_sample': 'depth'})\n",
    "    return combined_dataset\n",
    "\n",
    "def chunk_mask(combined_dataset, regions2d_list, date, chunk_ratio = 1.5, min_nonzero = 10):\n",
    "    # Cut image and mask into chunks equal to the height * chunk_ratio\n",
    "    ds_lengh = combined_dataset.sizes['ping_time']\n",
    "    chunk_size = int(combined_dataset.sizes['depth'] * chunk_ratio)\n",
    "    num_chunks = math.ceil(ds_lengh / chunk_size)\n",
    "    # Iterate over chunks, normalize each, overlay mask\n",
    "    for i in range(0, num_chunks):\n",
    "        start = i*chunk_size\n",
    "        end = min((i+1)*chunk_size, ds_lengh)\n",
    "        chunk = combined_dataset.isel(ping_time=slice(start,end))[\"Sv\"]\n",
    "        #print(chunk['ping_time'].min().values, chunk['ping_time'].max().values)\n",
    "        #print(chunk.values.T.shape)\n",
    "\n",
    "        # Overlay mask for this chunk\n",
    "        mask = None\n",
    "        for regions2d in regions2d_list:\n",
    "            region_mask_ds, region_points = regions2d.mask(\n",
    "                        chunk.isel(channel=1).drop_vars(\"channel\"),\n",
    "                        region_class=region_classes,\n",
    "                        collapse_to_2d = True\n",
    "                    )\n",
    "            if region_mask_ds:\n",
    "                loc_mask = region_mask_ds['mask_2d'].fillna(0).values.astype(int)\n",
    "                \n",
    "                # Replace region_id with class_id in the mask\n",
    "                region_class_mapping = regions2d.data.merge(region_classes_df, how='left', left_on = 'region_class', right_on = 'class')\n",
    "                region_class_mapping = region_class_mapping[region_class_mapping.region_id.isin(np.unique(loc_mask))][['region_id', 'class_ind']].astype(int).sort_values(by='region_id')\n",
    "                for _, row in region_class_mapping.iterrows():\n",
    "                    loc_mask[loc_mask == row.region_id] = row.class_ind\n",
    "                \n",
    "                if mask is not None and mask.size > 0:\n",
    "                    mask = mask + loc_mask\n",
    "                else:\n",
    "                    mask = loc_mask\n",
    "\n",
    "        # Only save chunks for which there is non-empty mask\n",
    "        if mask is not None and mask.size > 0:\n",
    "            print(i, chunk['ping_time'].min().values, chunk['ping_time'].max().values, np.count_nonzero(mask), np.unique(mask))\n",
    "            # check that there are enough annotated pixels\n",
    "            if np.count_nonzero(mask) < min_nonzero: continue\n",
    "\n",
    "            fname = '%s_%d.npy' % (date, i)\n",
    "    \n",
    "            chunk_filepath = os.path.join(img_dir, fname)\n",
    "            \n",
    "            chunk = normalize_each_channel(chunk.values.T)\n",
    "            np.save(chunk_filepath, chunk)\n",
    "            chunk = None\n",
    "\n",
    "            # binarize mask\n",
    "            binary_mask = (mask > 0).astype(int)\n",
    "            binary_mask_filepath = os.path.join(binary_mask_dir, fname)\n",
    "            np.save(binary_mask_filepath, binary_mask)\n",
    "            binary_mask = None     \n",
    "\n",
    "            # mask with class types\n",
    "            mask_filepath = os.path.join(mask_dir, fname)\n",
    "            np.save(mask_filepath, mask)\n",
    "            mask = None\n",
    "\n",
    "def process_one_day(zarr_paths, evr_paths, date, chunk_ratio = 1.5, min_nonzero = 10, ignore_vars = []):\n",
    "    combined_dataset = load_combine_process_zarrs(zarr_paths)\n",
    "\n",
    "    # there can be several evr files\n",
    "    regions2d_list = [er.read_evr(evr_file) for evr_file in evr_paths]\n",
    "\n",
    "    chunk_mask(combined_dataset, regions2d_list, date, chunk_ratio, min_nonzero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08204207-127c-4574-8623-f9005f752e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35548a1b2444c339e646949aed6543a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/cuda/.venv/lib/python3.11/site-packages/dask/array/core.py:4836: PerformanceWarning: Increasing number of chunks by factor of 10\n",
      "  result = blockwise(\n",
      "/home/ubuntu/cuda/.venv/lib/python3.11/site-packages/dask/array/core.py:4836: PerformanceWarning: Increasing number of chunks by factor of 10\n",
      "  result = blockwise(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 2007-07-01T04:09:42.938826000 2007-07-01T04:34:29.032574000 798 [0 3]\n",
      "11 2007-07-01T04:34:29.798199000 2007-07-01T04:59:17.516951000 2214 [0 3]\n",
      "12 2007-07-01T04:59:18.282574000 2007-07-01T05:24:04.001324000 903 [0 3]\n",
      "13 2007-07-01T05:24:04.766951000 2007-07-01T05:48:45.595074000 256 [0 2]\n",
      "14 2007-07-01T05:48:46.360701000 2007-07-01T06:13:26.063826000 241 [0 2 8]\n",
      "18 2007-07-01T07:27:13.532574000 2007-07-01T07:51:43.079449000 977 [0 2]\n",
      "19 2007-07-01T07:51:43.829449000 2007-07-01T08:16:13.954449000 135 [0 2]\n"
     ]
    }
   ],
   "source": [
    "region_classes=[\"Def herring\", \"Prob herring\", \"Poss herring\", \"Surface herring\", \"Mackerel\", \n",
    "                                                \"Gadoids\", \"Norway pout\", \"unidentified fish\"]\n",
    "region_classes_df = pd.DataFrame({'class': region_classes}, index=range(1, len(region_classes)+1))\n",
    "region_classes_df.to_csv(os.path.join(mask_dir, 'classes.csv'))\n",
    "region_classes_df['class_ind'] = region_classes_df.index.astype(int)\n",
    "\n",
    "ignore_vars = ['source_filenames', 'filenames', 'angle_offset_alongship', 'angle_offset_athwartship',\n",
    "                'beamwidth_alongship', 'beamwidth_athwartship', 'water_level', 'angle_sensitivity_alongship',\n",
    "               'angle_sensitivity_athwartship', 'equivalent_beam_angle', #frequency_nominal,\n",
    "               'gain_correction', 'sa_correction', 'sound_absorption', 'sound_speed'\n",
    "              ]\n",
    "\n",
    "# Process all datasets\n",
    "for date, rows in tqdm(zarr_evr_df.groupby('date'), total = len(zarr_evr_df.date.unique())):\n",
    "    if date == '0629' or date == '0630' or date == '0702': continue # Exclude broken datasets\n",
    "    zarr_paths = rows.zarr.values\n",
    "    evr_paths = [os.path.join(evr_dir, e) for e in np.unique(np.concatenate(rows.evr_files.values))]\n",
    "    process_one_day(zarr_paths, evr_paths, date, ignore_vars = ignore_vars)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cdd8d9-47f6-4000-a870-576523b0c0aa",
   "metadata": {},
   "source": [
    "# LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deb693ec-2891-46ad-a805-2ead59834775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import hflip\n",
    "def random_horizontal_flip(image, mask, p=0.5):\n",
    "    \"\"\"\n",
    "    Horizontally flip the given image and mask with a given probability.\n",
    "\n",
    "    Parameters:\n",
    "    - image: torch.Tensor, the input image tensor.\n",
    "    - mask: torch.Tensor, the input mask tensor.\n",
    "    - p: float, probability of the image and mask being flipped. Default is 0.5.\n",
    "\n",
    "    Returns:\n",
    "    - image: torch.Tensor, the potentially flipped image.\n",
    "    - mask: torch.Tensor, the potentially flipped mask.\n",
    "    \"\"\"\n",
    "    if torch.rand(1).item() < p:\n",
    "        image = hflip(image)\n",
    "        mask = hflip(mask)\n",
    "    return image, mask\n",
    "\n",
    "class SonarDataset(Dataset):\n",
    "    def __init__(self, data_paths, mask_paths, resize_size=512, num_patches_per_image=3):\n",
    "        \"\"\"\n",
    "        Initialize the SonarDataset.\n",
    "\n",
    "        Parameters:\n",
    "        - data_paths: list of str, paths to the image data files.\n",
    "        - mask_paths: list of str, paths to the mask data files.\n",
    "        - resize_size: int, the size to which the patches will be resized. Default is 512.\n",
    "        - num_patches_per_image: int, number of patches to extract from each image. Default is 3.\n",
    "        \"\"\"\n",
    "        self.data_paths = data_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.resize_size = resize_size\n",
    "        self.num_patches_per_image = num_patches_per_image\n",
    "        \n",
    "        # Transformations applied on the patches, NO IMAGENET\n",
    "        #self.transforms = transforms.Compose([\n",
    "        #    transforms.ToTensor(),\n",
    "        #])\n",
    "\n",
    "        # IMAGENET NORMALIZAZION\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.ToTensor(),  # Applies only if your data is not already a tensor\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of patches in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data_paths) * self.num_patches_per_image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a patch and its corresponding mask by index.\n",
    "\n",
    "        Parameters:\n",
    "        - idx: int, the index of the patch to retrieve.\n",
    "\n",
    "        Returns:\n",
    "        - image_patch: torch.Tensor, the transformed image patch.\n",
    "        - mask_patch: torch.Tensor, the transformed mask patch.\n",
    "        \"\"\"\n",
    "        file_idx = idx // self.num_patches_per_image\n",
    "        data_path = self.data_paths[file_idx]\n",
    "        mask_path = self.mask_paths[file_idx]\n",
    "\n",
    "        image = np.load(data_path, mmap_mode='r')\n",
    "        # take only first 3 channels\n",
    "        image = image[..., :3]\n",
    "        \n",
    "        mask = np.load(mask_path, mmap_mode='r')\n",
    "        patch_size = image.shape[0]\n",
    "\n",
    "        max_x = image.shape[1] - patch_size\n",
    "        if max_x <= 0:\n",
    "            raise ValueError(\"Patch size is larger than the image width.\")\n",
    "\n",
    "        x = random.randint(0, max_x)\n",
    "        image_patch = image[:, x:x + patch_size]\n",
    "        mask_patch = mask[:, x:x + patch_size]\n",
    "\n",
    "        if self.resize_size != patch_size:\n",
    "            image_patch = cv2.resize(image_patch, (self.resize_size, self.resize_size), interpolation=cv2.INTER_NEAREST)\n",
    "            mask_patch = cv2.resize(mask_patch, (self.resize_size, self.resize_size), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        image_patch = self.transforms(image_patch.astype(np.float32))\n",
    "        mask_patch = torch.tensor(mask_patch, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        image_patch, mask_patch = random_horizontal_flip(image_patch, mask_patch)\n",
    "\n",
    "        return image_patch, mask_patch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000161d3-9f87-4c1b-b655-60ffeda83625",
   "metadata": {},
   "source": [
    "## Split dataset intro Train/Test/Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1495a43f-6c03-4d7f-97a2-03bc0736f679",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = '/media/ubuntu/E/ML_data/imgs/'\n",
    "mask_dir = '/media/ubuntu/E/ML_data/binary_masks/'\n",
    "\n",
    "files = [f for f in os.listdir(img_dir) if f.endswith('.npy')]\n",
    "days = list(set([f.split('_')[0] for f in files]))\n",
    "temp_train_days, test_days = train_test_split(days, test_size=0.1, random_state=1)\n",
    "train_days, val_days = train_test_split(temp_train_days, test_size=0.2, random_state=1)\n",
    "\n",
    "train_images = [os.path.join(img_dir, f) for f in files if f.split('_')[0] in train_days]\n",
    "train_masks = [os.path.join(mask_dir, f) for f in files if f.split('_')[0] in train_days]\n",
    "\n",
    "val_images = [os.path.join(img_dir, f) for f in files if f.split('_')[0] in val_days]\n",
    "val_masks = [os.path.join(mask_dir, f) for f in files if f.split('_')[0] in val_days]\n",
    "\n",
    "test_images = [os.path.join(img_dir, f) for f in files if f.split('_')[0] in test_days]\n",
    "test_masks = [os.path.join(mask_dir, f) for f in files if f.split('_')[0] in test_days]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SonarDataset(train_images, train_masks)\n",
    "val_dataset = SonarDataset(val_images, val_masks)\n",
    "test_dataset = SonarDataset(test_images, test_masks)\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0674ab2b-9a0a-487f-9a6a-179b52f3d07a",
   "metadata": {},
   "source": [
    "# TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58c44f89-4f1a-49a6-94d7-f47403d49d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2, logits=True, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        alpha (tensor, optional): Weights for each class. Default is equal weight.\n",
    "        gamma (int, optional): Focusing parameter. Default is 2.\n",
    "        logits (bool, optional): If True, expects inputs as raw logits. If False, expects probabilities. Default is True.\n",
    "        reduction (str, optional): Specifies the reduction to apply to the output: 'none', 'mean', 'sum'. Default is 'mean'.\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha if alpha is not None else torch.tensor([1.0, 1.0])\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            # Compute the binary cross-entropy loss with logits\n",
    "            BCE_loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        else:\n",
    "            # Compute the binary cross-entropy loss\n",
    "            BCE_loss = torch.nn.functional.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        \n",
    "        # Ensure targets are on the same device as inputs\n",
    "        targets = targets.to(inputs.device).long()\n",
    "        # Ensure alpha is on the same device as targets\n",
    "        self.alpha = self.alpha.to(inputs.device)\n",
    "        # Dynamic alpha based on target class\n",
    "        alpha = self.alpha[targets]\n",
    "        \n",
    "        # Compute the modulating factor (1 - pt)^gamma\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        focal_loss = alpha * ((1 - pt) ** self.gamma) * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(focal_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(focal_loss)\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "class SegModel(pl.LightningModule):\n",
    "    def __init__(self, model, criterion, optimizer, threshold = 0.5):\n",
    "        \"\"\"\n",
    "        Initialize the SegModel.\n",
    "\n",
    "        Parameters:\n",
    "        - model: PyTorch model, the segmentation model to be used.\n",
    "        - criterion: loss function.\n",
    "        - optimizer: optimizer function.\n",
    "        - threshold: float, threshold for converting probabilities to binary predictions. Default is 0.5.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.threshold = threshold\n",
    "\n",
    "        # Initialize metrics\n",
    "        self.iou = MeanIoU(num_classes=2, per_class=True)  \n",
    "        self.precision = Precision(task=\"multiclass\", num_classes=2, average='none')\n",
    "        self.recall = Recall(task=\"multiclass\", num_classes=2, average='none')\n",
    "        self.f1 = F1Score(task=\"multiclass\", num_classes=2, average='none')\n",
    "        self.dice = GeneralizedDiceScore(num_classes=2, include_background=True, per_class=True)\n",
    "        self.confusion_matrix = ConfusionMatrix(task=\"multiclass\", num_classes=2)\n",
    "        self.auroc = AUROC(task=\"binary\")\n",
    "\n",
    "        self.test_outputs = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def shared_step(self, batch, stage):\n",
    "        \"\"\"\n",
    "        Shared step for training, validation.\n",
    "\n",
    "        Parameters:\n",
    "        - batch: the input batch containing images and masks.\n",
    "        - stage: str, the stage of the training process (e.g., \"train\", \"valid\").\n",
    "\n",
    "        Returns:\n",
    "        - dict: contains loss and IoU for the current batch.\n",
    "        \"\"\"\n",
    "        image, mask = batch\n",
    "        out = self.forward(image)\n",
    "        \n",
    "        # Ensure mask is float for focal loss compatibility\n",
    "        loss = self.criterion(out, mask.float())  \n",
    "        \n",
    "        # Convert logits to binary predictions\n",
    "        preds = (out.sigmoid() > 0.5).long()\n",
    "        \n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(preds, mask.long(), mode='binary')\n",
    "        iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n",
    "        \n",
    "        self.log(f\"{stage}_IoU\", iou, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(f\"{stage}_loss\", loss, prog_bar=True, on_step=False, on_epoch=True) \n",
    "        return {\"loss\": loss, \"iou\": iou}\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"train\")     \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"valid\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Test step to compute various metrics.\n",
    "\n",
    "        Parameters:\n",
    "        - batch: the input batch containing images and masks.\n",
    "        - batch_idx: int, the index of the current batch.\n",
    "\n",
    "        Returns:\n",
    "        - dict: contains various metrics for the current batch.\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        preds = torch.sigmoid(logits) > self.threshold\n",
    "        preds = preds.int()  # Convert boolean to integers\n",
    "        y = y.int()  # Ensure targets are also integers\n",
    "\n",
    "        self.precision.reset()\n",
    "        self.recall.reset()\n",
    "        self.f1.reset()\n",
    "        self.iou.reset()\n",
    "        \n",
    "        # Update metrics\n",
    "        iou_score = self.iou(preds, y)\n",
    "        dice_score = self.dice(preds, y)\n",
    "        precision = self.precision(preds, y)\n",
    "        recall = self.recall(preds, y)\n",
    "        f1 = self.f1(preds, y)\n",
    "        cm = self.confusion_matrix(preds, y).float()\n",
    "        auroc = self.auroc(preds, y)\n",
    "\n",
    "        outputs = {\n",
    "            \"iou\": iou_score,\n",
    "            \"dice\": dice_score,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"auroc\": auroc\n",
    "        }\n",
    "\n",
    "        self.test_outputs.append(outputs)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Aggregates metrics at the end of the test epoch and logs them.\n",
    "        \"\"\"\n",
    "        # Aggregate metrics\n",
    "        iou_scores = torch.stack([x['iou'] for x in self.test_outputs])\n",
    "        dice_scores = torch.stack([x['dice'] for x in self.test_outputs])\n",
    "        precisions = torch.stack([x['precision'] for x in self.test_outputs])\n",
    "        recalls = torch.stack([x['recall'] for x in self.test_outputs])\n",
    "        f1_scores = torch.stack([x['f1'] for x in self.test_outputs])\n",
    "        cm_scores = torch.stack([x['confusion_matrix'] for x in self.test_outputs])\n",
    "        auroc_scores = torch.stack([x['auroc'] for x in self.test_outputs])\n",
    "            \n",
    "        # Sum confusion matrices\n",
    "        sum_cm = cm_scores.sum(dim=0)\n",
    "        sum_cm_np = sum_cm.cpu().numpy()  # Convert to numpy array\n",
    "        cm_normalized = sum_cm_np.astype('float') / sum_cm_np.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        # Average metrics\n",
    "        avg_auroc = auroc_scores.mean()\n",
    "\n",
    "        avg_iou_background = iou_scores[:, 0].mean()\n",
    "        avg_iou_class_of_interest = iou_scores[:, 1].mean()\n",
    "\n",
    "        avg_dice_background = dice_scores[:, 0].mean()\n",
    "        avg_dice_class_of_interest = dice_scores[:, 1].mean()\n",
    "        \n",
    "        avg_precision_background = precisions[:, 0].mean()\n",
    "        avg_precision_class_of_interest = precisions[:, 1].mean()\n",
    "        avg_recall_background = recalls[:, 0].mean()\n",
    "        avg_recall_class_of_interest = recalls[:, 1].mean()\n",
    "        avg_f1_background = f1_scores[:, 0].mean()\n",
    "        avg_f1_class_of_interest = f1_scores[:, 1].mean()\n",
    "    \n",
    "        # Log aggregated metrics\n",
    "        self.log('avg_auroc', avg_auroc)\n",
    "        \n",
    "        self.log('avg_iou_background', avg_iou_background)\n",
    "        self.log('avg_iou_class_of_interest', avg_iou_class_of_interest)\n",
    "        \n",
    "        self.log('avg_dice_background', avg_dice_background)\n",
    "        self.log('avg_dice_class_of_interest', avg_dice_class_of_interest)\n",
    "        \n",
    "        self.log('avg_precision_background', avg_precision_background)\n",
    "        self.log('avg_precision_class_of_interest', avg_precision_class_of_interest)\n",
    "        self.log('avg_recall_background', avg_recall_background)\n",
    "        self.log('avg_recall_class_of_interest', avg_recall_class_of_interest)\n",
    "        self.log('avg_f1_background', avg_f1_background)\n",
    "        self.log('avg_f1_class_of_interest', avg_f1_class_of_interest)\n",
    "    \n",
    "        # Plot the normalized confusion matrix\n",
    "        plt.figure(figsize=(4, 3))\n",
    "        sns.heatmap(cm_normalized, annot=True, fmt='.4f', cmap='Blues', xticklabels=['BG', 'Fish'], yticklabels=['BG', 'Fish'])\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Normalized Confusion Matrix')\n",
    "        plt.show()\n",
    "    \n",
    "        # Clear the outputs for the next epoch\n",
    "        self.test_outputs = []\n",
    "\n",
    "    def select_threshold(self, dataloader, device='cuda'):\n",
    "        \"\"\"\n",
    "        Compute metrics for different probability thresholds to select the best threshold.\n",
    "\n",
    "        Parameters:\n",
    "        - dataloader: DataLoader, the DataLoader providing the images and masks.\n",
    "        - device: str, the device to use for computation. Default is 'cuda'.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: a DataFrame containing metrics for different thresholds.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        results = []\n",
    "\n",
    "        # Define probability thresholds\n",
    "        thresholds = torch.arange(0.1, 1, 0.1)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for threshold in tqdm(thresholds):\n",
    "                self.precision.reset()\n",
    "                self.recall.reset()\n",
    "                self.f1.reset()\n",
    "                self.iou.reset()\n",
    "                iou_list = []\n",
    "                \n",
    "                for batch_idx, batch in tqdm(enumerate(dataloader), leave=False):\n",
    "                    images, masks = batch\n",
    "                    images = images.to(device)\n",
    "                    masks = masks.to(device)\n",
    "                    # Make predictions\n",
    "                    logits = self.model(images)  \n",
    "                    # Binarize predictions\n",
    "                    preds = torch.sigmoid(logits) > threshold\n",
    "                    preds = preds.int()  # Convert boolean to integers\n",
    "                    masks = masks.int()  # Ensure targets are also integers\n",
    "    \n",
    "                    self.f1.update(preds, masks)\n",
    "                    self.precision.update(preds, masks)\n",
    "                    self.recall.update(preds, masks)\n",
    "                    # Update for IoU does not work correctly, so list is used instead\n",
    "                    #self.iou.update(preds, masks)\n",
    "                    iou_list.append(self.iou(preds, masks)[1].item())\n",
    "                    \n",
    "                average_precision = self.precision.compute()[1].item()\n",
    "                average_recall = self.recall.compute()[1].item()\n",
    "                average_f1 = self.f1.compute()[1].item()\n",
    "                #average_iou = self.iou.compute()[1].item()\n",
    "                average_iou = np.sum(iou_list) / len(iou_list)\n",
    "    \n",
    "                results.append({\n",
    "                    'Threshold': threshold.item(),\n",
    "                    'IoU': average_iou,\n",
    "                    'F1': average_f1,\n",
    "                    'Precision': average_precision,\n",
    "                    'Recall': average_recall\n",
    "                })\n",
    "    \n",
    "        df_results = pd.DataFrame(results)\n",
    "        \n",
    "        return df_results\n",
    "\n",
    "    def save_preds(self, logits, batch_idx, output_dir, file_prefix):\n",
    "        \"\"\"\n",
    "        Save predictions to the specified directory.\n",
    "\n",
    "        Parameters:\n",
    "        - logits: torch.Tensor, the logits output from the model.\n",
    "        - batch_idx: int, the index of the current batch.\n",
    "        - output_dir: str, directory where predictions will be saved.\n",
    "        - file_prefix: str, prefix for the output file names (optional).\n",
    "        \"\"\"\n",
    "        filename = str(batch_idx) + '.npy'\n",
    "        if file_prefix:\n",
    "            filename = file_prefix + '_' + filename\n",
    "        pred_file_path = os.path.join(output_dir, filename)\n",
    "        np.save(pred_file_path, logits.cpu())\n",
    "\n",
    "    def predict(self, dataloader, device='cuda', output_dir=None, file_prefix=None):\n",
    "        \"\"\"\n",
    "        Make predictions on a dataset and optionally save the results.\n",
    "\n",
    "        Parameters:\n",
    "        - dataloader: DataLoader, the DataLoader providing the images.\n",
    "        - device: str, the device to use for computation. Default is 'cuda'.\n",
    "        - output_dir: str, directory where predictions will be saved.\n",
    "        - file_prefix: str, prefix for the output file names.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        if output_dir is None: \n",
    "            print('No output dir for predictions specified.')\n",
    "            return\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "                images, _ = batch\n",
    "                images = images.to(device)\n",
    "                logits = self.model(images)\n",
    "                self.save_preds(logits, batch_idx, output_dir, file_prefix)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configure the optimizer and learning rate scheduler.\n",
    "\n",
    "        Returns:\n",
    "        - dict: containing the optimizer and learning rate scheduler.\n",
    "        \"\"\"\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.8, patience=10, min_lr=1e-05)\n",
    "        return {'optimizer': self.optimizer, 'lr_scheduler': scheduler, 'monitor': 'valid_loss'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fafc3d04-3598-4bc9-973d-9a802a3e8f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set matmul precision\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# Select model\n",
    "model_name = 'unet'\n",
    "encoder = 'resnet34'  \n",
    "pretrained = 'imagenet'\n",
    "\n",
    "# Name of the model\n",
    "run_name = model_name + '_' + encoder\n",
    "if pretrained: run_name += '_' + pretrained\n",
    "version_name = f\"{datetime.now().strftime('%d%m-%H%M')}\"\n",
    "run_name += '_' + version_name\n",
    "\n",
    "classes = 1\n",
    "\n",
    "# Create the model\n",
    "model = smp.create_model(model_name,\n",
    "                         encoder_name = encoder,\n",
    "                         in_channels = 3,\n",
    "                         encoder_weights=pretrained,\n",
    "                         classes = classes).to(device)\n",
    "   \n",
    "# Assuming class 0 is the background and class 1 is the class of interest\n",
    "# alpha shows relative importance of background vs class\n",
    "criterion = FocalLoss(alpha=torch.tensor([0.05, 0.95]), gamma=2.0, logits=True)\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-03)#, weight_decay=1e-05)\n",
    "\n",
    "# Set up how to save trained model weights\n",
    "# checkpoint_dir can be changed\n",
    "checkpoint_dir = f'./models/checkpoints_{run_name}'\n",
    "checkpoint = ModelCheckpoint(dirpath = checkpoint_dir,\n",
    "                                   #filename='{epoch:02d}-{valid_IoU:.2f}',\n",
    "                                   filename='best_model', \n",
    "                                   save_top_k=1,\n",
    "                                   verbose = True, \n",
    "                                   monitor = 'valid_loss', \n",
    "                                   mode = 'min')\n",
    "# Set up early training stopping\n",
    "early_stopping = EarlyStopping(monitor='valid_loss', patience=10, mode='min')\n",
    "# How often update learning rate\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15d75594-f5bd-45a6-bf41-47018f296289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: tb_logs/unet_resnet34_imagenet_1807-1008\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                      | Params\n",
      "---------------------------------------------------------------\n",
      "0 | model            | Unet                      | 24.4 M\n",
      "1 | criterion        | FocalLoss                 | 0     \n",
      "2 | iou              | MeanIoU                   | 0     \n",
      "3 | precision        | MulticlassPrecision       | 0     \n",
      "4 | recall           | MulticlassRecall          | 0     \n",
      "5 | f1               | MulticlassF1Score         | 0     \n",
      "6 | dice             | GeneralizedDiceScore      | 0     \n",
      "7 | confusion_matrix | MulticlassConfusionMatrix | 0     \n",
      "8 | auroc            | BinaryAUROC               | 0     \n",
      "---------------------------------------------------------------\n",
      "24.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.4 M    Total params\n",
      "97.745    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/cuda/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1459a20d60b24d44877c5967dcc8f485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/cuda/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "pl_model = SegModel(model, criterion, optimizer)\n",
    "\n",
    "# Select profiler\n",
    "profiler = SimpleProfiler()\n",
    "\n",
    "# Initialize the logger\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=\"tb_logs\",\n",
    "    name=f'{run_name}'\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = pl.Trainer(\n",
    "    profiler=profiler,\n",
    "    num_sanity_val_steps=5,\n",
    "    logger=logger,\n",
    "    gradient_clip_val=0.5,\n",
    "    precision='16-mixed',\n",
    "    accelerator='gpu',\n",
    "    max_epochs=100,\n",
    "    callbacks=[checkpoint, early_stopping, lr_monitor],\n",
    "    #val_check_interval=0.1  # Validate more frequently\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(pl_model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c344915-97df-4888-af7e-21dad482f1f1",
   "metadata": {},
   "source": [
    "# TEST TRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e833c3-ecd5-47c1-8afb-6f5e66b278f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "av_results = pd.DataFrame()\n",
    "# Since test set uses augmentation, we will average over 10 runs\n",
    "# Averaged results will be displayed at the end\n",
    "for i in tqdm(range(0, 10)):\n",
    "    results = trainer.test(pl_model, test_loader)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    av_results = pd.concat([av_results, pd.DataFrame(results[0], index=[i])])\n",
    "av_results.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
