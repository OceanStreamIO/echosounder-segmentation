{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ab3aaac-20fc-44ae-8506-99c7ff9ab832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import transforms\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch import Unet\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "from segmentation_models_pytorch.utils import train as smp_train\n",
    "from segmentation_models_pytorch import utils\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.profilers import AdvancedProfiler, SimpleProfiler\n",
    "\n",
    "from torchmetrics.segmentation import MeanIoU, GeneralizedDiceScore\n",
    "from torchmetrics import Accuracy, F1Score, Precision, Recall, ConfusionMatrix, AUROC\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f72cf0ca-c643-4890-8e4e-39bc9680b44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfcc3e4-88f7-435b-b269-7d06a4827b4f",
   "metadata": {},
   "source": [
    "# SET UP INPUT/OUTPUT PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "860e05fc-06ec-40e4-9794-5a3786a9884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evr_dir = '/media/ubuntu/E/EVR_region_files'\n",
    "\n",
    "img_dir = '/media/ubuntu/E/ML_data/imgs'\n",
    "mask_dir = '/media/ubuntu/E/ML_data/masks'\n",
    "binary_mask_dir = '/media/ubuntu/E/ML_data/binary_masks'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea6d1e3-55d0-438b-905a-34b1d46f69ec",
   "metadata": {},
   "source": [
    "# COLLECT ZAR PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e24cdd8b-a5f5-4779-a654-0315082c798b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Find EVR for each ZARR \u001b[39;00m\n\u001b[1;32m      2\u001b[0m zarr_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/media/ubuntu/E/processed\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m zarr_evr_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzarr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevr_files\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m zarr_subdir \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(zarr_dir):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "import echoregions as er\n",
    "\n",
    "# Find EVR for each ZARR \n",
    "zarr_dir = '/media/ubuntu/E/processed'\n",
    "zarr_evr_df = pd.DataFrame(columns = ['date', 'zarr', 'evr_files'])\n",
    "counter = 0\n",
    "for zarr_subdir in os.listdir(zarr_dir):\n",
    "    for zarr_file in os.listdir(os.path.join(zarr_dir, zarr_subdir)):\n",
    "        _, date, _ = zarr_file.split('-')\n",
    "        date = date[5:]\n",
    "        month = date[:2]\n",
    "        day = date[2:4]\n",
    "        if month == '06': month = 'June'\n",
    "        elif month == '07': month = 'July'\n",
    "        else: \n",
    "            print(date, 'Different month', month)\n",
    "        evr_date = day + month\n",
    "        evr_files = []\n",
    "        for evr_file in os.listdir(evr_dir):\n",
    "            if evr_file.startswith(evr_date):\n",
    "                evr_files.append(evr_file)\n",
    "    \n",
    "        zarr_file = os.path.join(zarr_dir, zarr_subdir, zarr_file, zarr_file + '_Sv.zarr')\n",
    "        zarr_evr_df.loc[counter] = [date, zarr_file, evr_files]\n",
    "        counter += 1\n",
    "\n",
    "zarr_evr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4627c62-9075-4cdc-b994-b3ce4733bcf2",
   "metadata": {},
   "source": [
    "# CUT ALL DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7335ac5-6a9a-4384-82cc-276f1ceef23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Returning No Mask. Empty 3D Mask cannot be converted to 2D Mask.\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"No gridpoint belongs to any region.\")\n",
    "\n",
    "chunk_sizes = {\n",
    "    'channel': -1,           # Load all channels in one chunk\n",
    "    'ping_time': 100,        # Chunk by 100\n",
    "    'range_sample': 650      # Split into 2 chunks\n",
    "}\n",
    "\n",
    "def find_nan_depths(channel_data):\n",
    "    nan_mask = channel_data.isnull()\n",
    "    all_nans = nan_mask.all(dim='ping_time')\n",
    "    all_nan_depths = channel_data.range_sample.where(all_nans, drop=True).values\n",
    "    return all_nan_depths\n",
    "\n",
    "# Use Dask\n",
    "def load_zarr_lazy(zarr_path, chunk_sizes=chunk_sizes, ignore_vars = []):\n",
    "    return xr.open_zarr(zarr_path, chunks=chunk_sizes, drop_variables = ignore_vars)   \n",
    "\n",
    "def correct_echo_range(ds):\n",
    "    # Replace channel and ping_time with their first elements\n",
    "    first_channel = ds[\"channel\"].values[0]\n",
    "    first_ping_time = ds[\"ping_time\"].values[0]\n",
    "    \n",
    "    # Slice the echo_range to get the desired range of values\n",
    "    selected_echo_range = ds[\"echo_range\"].sel(channel=first_channel, ping_time=first_ping_time)\n",
    "    selected_echo_range = selected_echo_range.values.tolist()\n",
    "    selected_echo_range = [value + 8.6 for value in selected_echo_range]\n",
    "\n",
    "    # Find min and max ignoring NaNs\n",
    "    min_val = np.nanmin(selected_echo_range)\n",
    "    max_val = np.nanmax(selected_echo_range)\n",
    "    \n",
    "    # Assign the values to the depth coordinate, transducer offset 8.6m\n",
    "    ds = ds.assign_coords(range_sample=selected_echo_range)\n",
    "\n",
    "    # Remove nan values\n",
    "    ds = ds.sel(range_sample=slice(min_val, max_val))\n",
    "    \n",
    "    return ds\n",
    "\n",
    "def normalize_each_channel(data):\n",
    "    # Replace NaNs with the minimum value - 10 for each channel\n",
    "    min_vals = np.nanmin(data, axis=(0, 1), keepdims=True)\n",
    "    data = np.where(np.isnan(data), min_vals - 10, data)\n",
    "\n",
    "    # Calculate the minimum value for each channel\n",
    "    min_vals = np.nanmin(data, axis=(0, 1), keepdims=True)\n",
    "    max_vals = np.nanmax(data, axis=(0, 1), keepdims=True)\n",
    "    \n",
    "    # Calculate normalization parameters\n",
    "    ranges = max_vals - min_vals\n",
    "    ranges[ranges == 0] = 1\n",
    "    \n",
    "    # Normalize the data\n",
    "    data_normalized = (data - min_vals) / ranges\n",
    "    return data_normalized\n",
    "\n",
    "def fill_na_with_interpolation(chunk):\n",
    "    for channel in chunk.channel:\n",
    "        chunk.loc[dict(channel=channel)] = chunk.sel(channel=channel).interpolate_na(dim='ping_time', method='linear')\n",
    "    return chunk\n",
    "\n",
    "def load_combine_process_zarrs(zarr_paths, ignore_vars = []):\n",
    "    datasets = [load_zarr_lazy(path, chunk_sizes = {}, ignore_vars = ignore_vars) for path in zarr_paths]\n",
    "    combined_dataset = xr.concat(datasets, dim='ping_time')\n",
    "    combined_dataset = combined_dataset.sortby('ping_time')\n",
    "    # select first 3 channels\n",
    "    combined_dataset = combined_dataset.isel(channel=slice(0, 3))\n",
    "    # remove empty pings\n",
    "    combined_dataset = combined_dataset.dropna(dim='ping_time', how='all', subset=['Sv'])\n",
    "    combined_dataset = correct_echo_range(combined_dataset)\n",
    "    #combined_dataset = apply_remove_background_noise(combined_dataset)\n",
    "    combined_dataset = combined_dataset.rename({'range_sample': 'depth'})\n",
    "    return combined_dataset\n",
    "\n",
    "def chunk_mask(combined_dataset, regions2d_list, date, chunk_ratio = 1.5, min_nonzero = 10):\n",
    "    # Cut image and mask into chunks equal to the height * chunk_ratio\n",
    "    ds_lengh = combined_dataset.sizes['ping_time']\n",
    "    chunk_size = int(combined_dataset.sizes['depth'] * chunk_ratio)\n",
    "    num_chunks = math.ceil(ds_lengh / chunk_size)\n",
    "    # Iterate over chunks, normalize each, overlay mask\n",
    "    for i in range(0, num_chunks):\n",
    "        start = i*chunk_size\n",
    "        end = min((i+1)*chunk_size, ds_lengh)\n",
    "        chunk = combined_dataset.isel(ping_time=slice(start,end))[\"Sv\"]\n",
    "        #print(chunk['ping_time'].min().values, chunk['ping_time'].max().values)\n",
    "        #print(chunk.values.T.shape)\n",
    "\n",
    "        # Overlay mask for this chunk\n",
    "        mask = None\n",
    "        for regions2d in regions2d_list:\n",
    "            region_mask_ds, region_points = regions2d.mask(\n",
    "                        chunk.isel(channel=1).drop_vars(\"channel\"),\n",
    "                        region_class=region_classes,\n",
    "                        collapse_to_2d = True\n",
    "                    )\n",
    "            if region_mask_ds:\n",
    "                loc_mask = region_mask_ds['mask_2d'].fillna(0).values.astype(int)\n",
    "                \n",
    "                # Replace region_id with class_id in the mask\n",
    "                region_class_mapping = regions2d.data.merge(region_classes_df, how='left', left_on = 'region_class', right_on = 'class')\n",
    "                region_class_mapping = region_class_mapping[region_class_mapping.region_id.isin(np.unique(loc_mask))][['region_id', 'class_ind']].astype(int).sort_values(by='region_id')\n",
    "                for _, row in region_class_mapping.iterrows():\n",
    "                    loc_mask[loc_mask == row.region_id] = row.class_ind\n",
    "                \n",
    "                if mask is not None and mask.size > 0:\n",
    "                    mask = mask + loc_mask\n",
    "                else:\n",
    "                    mask = loc_mask\n",
    "\n",
    "        # Only save chunks for which there is non-empty mask\n",
    "        if mask is not None and mask.size > 0:\n",
    "            print(i, chunk['ping_time'].min().values, chunk['ping_time'].max().values, np.count_nonzero(mask), np.unique(mask))\n",
    "            # check that there are enough annotated pixels\n",
    "            if np.count_nonzero(mask) < min_nonzero: continue\n",
    "\n",
    "            fname = '%s_%d.npy' % (date, i)\n",
    "    \n",
    "            chunk_filepath = os.path.join(img_dir, fname)\n",
    "            \n",
    "            chunk = normalize_each_channel(chunk.values.T)\n",
    "            np.save(chunk_filepath, chunk)\n",
    "            chunk = None\n",
    "\n",
    "            # binarize mask\n",
    "            binary_mask = (mask > 0).astype(int)\n",
    "            binary_mask_filepath = os.path.join(binary_mask_dir, fname)\n",
    "            np.save(binary_mask_filepath, binary_mask)\n",
    "            binary_mask = None     \n",
    "\n",
    "            # mask with class types\n",
    "            mask_filepath = os.path.join(mask_dir, fname)\n",
    "            np.save(mask_filepath, mask)\n",
    "            mask = None\n",
    "\n",
    "def process_one_day(zarr_paths, evr_paths, date, chunk_ratio = 1.5, min_nonzero = 10, ignore_vars = []):\n",
    "    combined_dataset = load_combine_process_zarrs(zarr_paths)\n",
    "\n",
    "    # there can be several evr files\n",
    "    regions2d_list = [er.read_evr(evr_file) for evr_file in evr_paths]\n",
    "\n",
    "    chunk_mask(combined_dataset, regions2d_list, date, chunk_ratio, min_nonzero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08204207-127c-4574-8623-f9005f752e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35548a1b2444c339e646949aed6543a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/cuda/.venv/lib/python3.11/site-packages/dask/array/core.py:4836: PerformanceWarning: Increasing number of chunks by factor of 10\n",
      "  result = blockwise(\n",
      "/home/ubuntu/cuda/.venv/lib/python3.11/site-packages/dask/array/core.py:4836: PerformanceWarning: Increasing number of chunks by factor of 10\n",
      "  result = blockwise(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 2007-07-01T04:09:42.938826000 2007-07-01T04:34:29.032574000 798 [0 3]\n",
      "11 2007-07-01T04:34:29.798199000 2007-07-01T04:59:17.516951000 2214 [0 3]\n",
      "12 2007-07-01T04:59:18.282574000 2007-07-01T05:24:04.001324000 903 [0 3]\n",
      "13 2007-07-01T05:24:04.766951000 2007-07-01T05:48:45.595074000 256 [0 2]\n",
      "14 2007-07-01T05:48:46.360701000 2007-07-01T06:13:26.063826000 241 [0 2 8]\n",
      "18 2007-07-01T07:27:13.532574000 2007-07-01T07:51:43.079449000 977 [0 2]\n",
      "19 2007-07-01T07:51:43.829449000 2007-07-01T08:16:13.954449000 135 [0 2]\n"
     ]
    }
   ],
   "source": [
    "region_classes=[\"Def herring\", \"Prob herring\", \"Poss herring\", \"Surface herring\", \"Mackerel\", \n",
    "                                                \"Gadoids\", \"Norway pout\", \"unidentified fish\"]\n",
    "region_classes_df = pd.DataFrame({'class': region_classes}, index=range(1, len(region_classes)+1))\n",
    "region_classes_df.to_csv(os.path.join(mask_dir, 'classes.csv'))\n",
    "region_classes_df['class_ind'] = region_classes_df.index.astype(int)\n",
    "\n",
    "ignore_vars = ['source_filenames', 'filenames', 'angle_offset_alongship', 'angle_offset_athwartship',\n",
    "                'beamwidth_alongship', 'beamwidth_athwartship', 'water_level', 'angle_sensitivity_alongship',\n",
    "               'angle_sensitivity_athwartship', 'equivalent_beam_angle', #frequency_nominal,\n",
    "               'gain_correction', 'sa_correction', 'sound_absorption', 'sound_speed'\n",
    "              ]\n",
    "\n",
    "# Process all datasets\n",
    "for date, rows in tqdm(zarr_evr_df.groupby('date'), total = len(zarr_evr_df.date.unique())):\n",
    "    if date == '0629' or date == '0630' or date == '0702': continue # Exclude broken datasets\n",
    "    zarr_paths = rows.zarr.values\n",
    "    evr_paths = [os.path.join(evr_dir, e) for e in np.unique(np.concatenate(rows.evr_files.values))]\n",
    "    process_one_day(zarr_paths, evr_paths, date, ignore_vars = ignore_vars)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cdd8d9-47f6-4000-a870-576523b0c0aa",
   "metadata": {},
   "source": [
    "# LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deb693ec-2891-46ad-a805-2ead59834775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import hflip\n",
    "def random_horizontal_flip(image, mask, p=0.5):\n",
    "    \"\"\"\n",
    "    Horizontally flip the given image and mask with a given probability.\n",
    "\n",
    "    Parameters:\n",
    "    - image: torch.Tensor, the input image tensor.\n",
    "    - mask: torch.Tensor, the input mask tensor.\n",
    "    - p: float, probability of the image and mask being flipped. Default is 0.5.\n",
    "\n",
    "    Returns:\n",
    "    - image: torch.Tensor, the potentially flipped image.\n",
    "    - mask: torch.Tensor, the potentially flipped mask.\n",
    "    \"\"\"\n",
    "    if torch.rand(1).item() < p:\n",
    "        image = hflip(image)\n",
    "        mask = hflip(mask)\n",
    "    return image, mask\n",
    "\n",
    "class SonarDataset(Dataset):\n",
    "    def __init__(self, data_paths, mask_paths, resize_size=512, num_patches_per_image=3):\n",
    "        \"\"\"\n",
    "        Initialize the SonarDataset.\n",
    "\n",
    "        Parameters:\n",
    "        - data_paths: list of str, paths to the image data files.\n",
    "        - mask_paths: list of str, paths to the mask data files.\n",
    "        - resize_size: int, the size to which the patches will be resized. Default is 512.\n",
    "        - num_patches_per_image: int, number of patches to extract from each image. Default is 3.\n",
    "        \"\"\"\n",
    "        self.data_paths = data_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.resize_size = resize_size\n",
    "        self.num_patches_per_image = num_patches_per_image\n",
    "        \n",
    "        # Transformations applied on the patches, NO IMAGENET\n",
    "        #self.transforms = transforms.Compose([\n",
    "        #    transforms.ToTensor(),\n",
    "        #])\n",
    "\n",
    "        # IMAGENET NORMALIZAZION\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.ToTensor(),  # Applies only if your data is not already a tensor\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of patches in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data_paths) * self.num_patches_per_image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a patch and its corresponding mask by index.\n",
    "\n",
    "        Parameters:\n",
    "        - idx: int, the index of the patch to retrieve.\n",
    "\n",
    "        Returns:\n",
    "        - image_patch: torch.Tensor, the transformed image patch.\n",
    "        - mask_patch: torch.Tensor, the transformed mask patch.\n",
    "        \"\"\"\n",
    "        file_idx = idx // self.num_patches_per_image\n",
    "        data_path = self.data_paths[file_idx]\n",
    "        mask_path = self.mask_paths[file_idx]\n",
    "\n",
    "        image = np.load(data_path, mmap_mode='r')\n",
    "        # take only first 3 channels\n",
    "        image = image[..., :3]\n",
    "        \n",
    "        mask = np.load(mask_path, mmap_mode='r')\n",
    "        patch_size = image.shape[0]\n",
    "\n",
    "        max_x = image.shape[1] - patch_size\n",
    "        if max_x <= 0:\n",
    "            raise ValueError(\"Patch size is larger than the image width.\")\n",
    "\n",
    "        x = random.randint(0, max_x)\n",
    "        image_patch = image[:, x:x + patch_size]\n",
    "        mask_patch = mask[:, x:x + patch_size]\n",
    "\n",
    "        if self.resize_size != patch_size:\n",
    "            image_patch = cv2.resize(image_patch, (self.resize_size, self.resize_size), interpolation=cv2.INTER_NEAREST)\n",
    "            mask_patch = cv2.resize(mask_patch, (self.resize_size, self.resize_size), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        image_patch = self.transforms(image_patch.astype(np.float32))\n",
    "        mask_patch = torch.tensor(mask_patch, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        image_patch, mask_patch = random_horizontal_flip(image_patch, mask_patch)\n",
    "\n",
    "        return image_patch, mask_patch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000161d3-9f87-4c1b-b655-60ffeda83625",
   "metadata": {},
   "source": [
    "## Split dataset intro Train/Test/Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1495a43f-6c03-4d7f-97a2-03bc0736f679",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = '/media/ubuntu/E/ML_data/imgs/'\n",
    "mask_dir = '/media/ubuntu/E/ML_data/binary_masks/'\n",
    "\n",
    "files = [f for f in os.listdir(img_dir) if f.endswith('.npy')]\n",
    "days = list(set([f.split('_')[0] for f in files]))\n",
    "temp_train_days, test_days = train_test_split(days, test_size=0.1, random_state=1)\n",
    "train_days, val_days = train_test_split(temp_train_days, test_size=0.2, random_state=1)\n",
    "\n",
    "train_images = [os.path.join(img_dir, f) for f in files if f.split('_')[0] in train_days]\n",
    "train_masks = [os.path.join(mask_dir, f) for f in files if f.split('_')[0] in train_days]\n",
    "\n",
    "val_images = [os.path.join(img_dir, f) for f in files if f.split('_')[0] in val_days]\n",
    "val_masks = [os.path.join(mask_dir, f) for f in files if f.split('_')[0] in val_days]\n",
    "\n",
    "test_images = [os.path.join(img_dir, f) for f in files if f.split('_')[0] in test_days]\n",
    "test_masks = [os.path.join(mask_dir, f) for f in files if f.split('_')[0] in test_days]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SonarDataset(train_images, train_masks)\n",
    "val_dataset = SonarDataset(val_images, val_masks)\n",
    "test_dataset = SonarDataset(test_images, test_masks)\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0674ab2b-9a0a-487f-9a6a-179b52f3d07a",
   "metadata": {},
   "source": [
    "# TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58c44f89-4f1a-49a6-94d7-f47403d49d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2, logits=True, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        alpha (tensor, optional): Weights for each class. Default is equal weight.\n",
    "        gamma (int, optional): Focusing parameter. Default is 2.\n",
    "        logits (bool, optional): If True, expects inputs as raw logits. If False, expects probabilities. Default is True.\n",
    "        reduction (str, optional): Specifies the reduction to apply to the output: 'none', 'mean', 'sum'. Default is 'mean'.\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha if alpha is not None else torch.tensor([1.0, 1.0])\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            # Compute the binary cross-entropy loss with logits\n",
    "            BCE_loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        else:\n",
    "            # Compute the binary cross-entropy loss\n",
    "            BCE_loss = torch.nn.functional.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        \n",
    "        # Ensure targets are on the same device as inputs\n",
    "        targets = targets.to(inputs.device).long()\n",
    "        # Ensure alpha is on the same device as targets\n",
    "        self.alpha = self.alpha.to(inputs.device)\n",
    "        # Dynamic alpha based on target class\n",
    "        alpha = self.alpha[targets]\n",
    "        \n",
    "        # Compute the modulating factor (1 - pt)^gamma\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        focal_loss = alpha * ((1 - pt) ** self.gamma) * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(focal_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(focal_loss)\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "class SegModel(pl.LightningModule):\n",
    "    def __init__(self, model, criterion, optimizer, threshold = 0.5):\n",
    "        \"\"\"\n",
    "        Initialize the SegModel.\n",
    "\n",
    "        Parameters:\n",
    "        - model: PyTorch model, the segmentation model to be used.\n",
    "        - criterion: loss function.\n",
    "        - optimizer: optimizer function.\n",
    "        - threshold: float, threshold for converting probabilities to binary predictions. Default is 0.5.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.threshold = threshold\n",
    "\n",
    "        # Initialize metrics\n",
    "        self.iou = MeanIoU(num_classes=2, per_class=True)  \n",
    "        self.precision = Precision(task=\"multiclass\", num_classes=2, average='none')\n",
    "        self.recall = Recall(task=\"multiclass\", num_classes=2, average='none')\n",
    "        self.f1 = F1Score(task=\"multiclass\", num_classes=2, average='none')\n",
    "        self.dice = GeneralizedDiceScore(num_classes=2, include_background=True, per_class=True)\n",
    "        self.confusion_matrix = ConfusionMatrix(task=\"multiclass\", num_classes=2)\n",
    "        self.auroc = AUROC(task=\"binary\")\n",
    "\n",
    "        self.test_outputs = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def shared_step(self, batch, stage):\n",
    "        \"\"\"\n",
    "        Shared step for training, validation.\n",
    "\n",
    "        Parameters:\n",
    "        - batch: the input batch containing images and masks.\n",
    "        - stage: str, the stage of the training process (e.g., \"train\", \"valid\").\n",
    "\n",
    "        Returns:\n",
    "        - dict: contains loss and IoU for the current batch.\n",
    "        \"\"\"\n",
    "        image, mask = batch\n",
    "        out = self.forward(image)\n",
    "        \n",
    "        # Ensure mask is float for focal loss compatibility\n",
    "        loss = self.criterion(out, mask.float())  \n",
    "        \n",
    "        # Convert logits to binary predictions\n",
    "        preds = (out.sigmoid() > 0.5).long()\n",
    "        \n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(preds, mask.long(), mode='binary')\n",
    "        iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n",
    "        \n",
    "        self.log(f\"{stage}_IoU\", iou, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(f\"{stage}_loss\", loss, prog_bar=True, on_step=False, on_epoch=True) \n",
    "        return {\"loss\": loss, \"iou\": iou}\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"train\")     \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"valid\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Test step to compute various metrics.\n",
    "\n",
    "        Parameters:\n",
    "        - batch: the input batch containing images and masks.\n",
    "        - batch_idx: int, the index of the current batch.\n",
    "\n",
    "        Returns:\n",
    "        - dict: contains various metrics for the current batch.\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        preds = torch.sigmoid(logits) > self.threshold\n",
    "        preds = preds.int()  # Convert boolean to integers\n",
    "        y = y.int()  # Ensure targets are also integers\n",
    "\n",
    "        self.precision.reset()\n",
    "        self.recall.reset()\n",
    "        self.f1.reset()\n",
    "        self.iou.reset()\n",
    "        \n",
    "        # Update metrics\n",
    "        iou_score = self.iou(preds, y)\n",
    "        dice_score = self.dice(preds, y)\n",
    "        precision = self.precision(preds, y)\n",
    "        recall = self.recall(preds, y)\n",
    "        f1 = self.f1(preds, y)\n",
    "        cm = self.confusion_matrix(preds, y).float()\n",
    "        auroc = self.auroc(preds, y)\n",
    "\n",
    "        outputs = {\n",
    "            \"iou\": iou_score,\n",
    "            \"dice\": dice_score,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"auroc\": auroc\n",
    "        }\n",
    "\n",
    "        self.test_outputs.append(outputs)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Aggregates metrics at the end of the test epoch and logs them.\n",
    "        \"\"\"\n",
    "        # Aggregate metrics\n",
    "        iou_scores = torch.stack([x['iou'] for x in self.test_outputs])\n",
    "        dice_scores = torch.stack([x['dice'] for x in self.test_outputs])\n",
    "        precisions = torch.stack([x['precision'] for x in self.test_outputs])\n",
    "        recalls = torch.stack([x['recall'] for x in self.test_outputs])\n",
    "        f1_scores = torch.stack([x['f1'] for x in self.test_outputs])\n",
    "        cm_scores = torch.stack([x['confusion_matrix'] for x in self.test_outputs])\n",
    "        auroc_scores = torch.stack([x['auroc'] for x in self.test_outputs])\n",
    "            \n",
    "        # Sum confusion matrices\n",
    "        sum_cm = cm_scores.sum(dim=0)\n",
    "        sum_cm_np = sum_cm.cpu().numpy()  # Convert to numpy array\n",
    "        cm_normalized = sum_cm_np.astype('float') / sum_cm_np.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        # Average metrics\n",
    "        avg_auroc = auroc_scores.mean()\n",
    "\n",
    "        avg_iou_background = iou_scores[:, 0].mean()\n",
    "        avg_iou_class_of_interest = iou_scores[:, 1].mean()\n",
    "\n",
    "        avg_dice_background = dice_scores[:, 0].mean()\n",
    "        avg_dice_class_of_interest = dice_scores[:, 1].mean()\n",
    "        \n",
    "        avg_precision_background = precisions[:, 0].mean()\n",
    "        avg_precision_class_of_interest = precisions[:, 1].mean()\n",
    "        avg_recall_background = recalls[:, 0].mean()\n",
    "        avg_recall_class_of_interest = recalls[:, 1].mean()\n",
    "        avg_f1_background = f1_scores[:, 0].mean()\n",
    "        avg_f1_class_of_interest = f1_scores[:, 1].mean()\n",
    "    \n",
    "        # Log aggregated metrics\n",
    "        self.log('avg_auroc', avg_auroc)\n",
    "        \n",
    "        self.log('avg_iou_background', avg_iou_background)\n",
    "        self.log('avg_iou_class_of_interest', avg_iou_class_of_interest)\n",
    "        \n",
    "        self.log('avg_dice_background', avg_dice_background)\n",
    "        self.log('avg_dice_class_of_interest', avg_dice_class_of_interest)\n",
    "        \n",
    "        self.log('avg_precision_background', avg_precision_background)\n",
    "        self.log('avg_precision_class_of_interest', avg_precision_class_of_interest)\n",
    "        self.log('avg_recall_background', avg_recall_background)\n",
    "        self.log('avg_recall_class_of_interest', avg_recall_class_of_interest)\n",
    "        self.log('avg_f1_background', avg_f1_background)\n",
    "        self.log('avg_f1_class_of_interest', avg_f1_class_of_interest)\n",
    "    \n",
    "        # Plot the normalized confusion matrix\n",
    "        plt.figure(figsize=(4, 3))\n",
    "        sns.heatmap(cm_normalized, annot=True, fmt='.4f', cmap='Blues', xticklabels=['BG', 'Fish'], yticklabels=['BG', 'Fish'])\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Normalized Confusion Matrix')\n",
    "        plt.show()\n",
    "    \n",
    "        # Clear the outputs for the next epoch\n",
    "        self.test_outputs = []\n",
    "\n",
    "    def select_threshold(self, dataloader, device='cuda'):\n",
    "        \"\"\"\n",
    "        Compute metrics for different probability thresholds to select the best threshold.\n",
    "\n",
    "        Parameters:\n",
    "        - dataloader: DataLoader, the DataLoader providing the images and masks.\n",
    "        - device: str, the device to use for computation. Default is 'cuda'.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: a DataFrame containing metrics for different thresholds.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        results = []\n",
    "\n",
    "        # Define probability thresholds\n",
    "        thresholds = torch.arange(0.1, 1, 0.1)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for threshold in tqdm(thresholds):\n",
    "                self.precision.reset()\n",
    "                self.recall.reset()\n",
    "                self.f1.reset()\n",
    "                self.iou.reset()\n",
    "                iou_list = []\n",
    "                \n",
    "                for batch_idx, batch in tqdm(enumerate(dataloader), leave=False):\n",
    "                    images, masks = batch\n",
    "                    images = images.to(device)\n",
    "                    masks = masks.to(device)\n",
    "                    # Make predictions\n",
    "                    logits = self.model(images)  \n",
    "                    # Binarize predictions\n",
    "                    preds = torch.sigmoid(logits) > threshold\n",
    "                    preds = preds.int()  # Convert boolean to integers\n",
    "                    masks = masks.int()  # Ensure targets are also integers\n",
    "    \n",
    "                    self.f1.update(preds, masks)\n",
    "                    self.precision.update(preds, masks)\n",
    "                    self.recall.update(preds, masks)\n",
    "                    # Update for IoU does not work correctly, so list is used instead\n",
    "                    #self.iou.update(preds, masks)\n",
    "                    iou_list.append(self.iou(preds, masks)[1].item())\n",
    "                    \n",
    "                average_precision = self.precision.compute()[1].item()\n",
    "                average_recall = self.recall.compute()[1].item()\n",
    "                average_f1 = self.f1.compute()[1].item()\n",
    "                #average_iou = self.iou.compute()[1].item()\n",
    "                average_iou = np.sum(iou_list) / len(iou_list)\n",
    "    \n",
    "                results.append({\n",
    "                    'Threshold': threshold.item(),\n",
    "                    'IoU': average_iou,\n",
    "                    'F1': average_f1,\n",
    "                    'Precision': average_precision,\n",
    "                    'Recall': average_recall\n",
    "                })\n",
    "    \n",
    "        df_results = pd.DataFrame(results)\n",
    "        \n",
    "        return df_results\n",
    "\n",
    "    def save_preds(self, logits, batch_idx, output_dir, file_prefix):\n",
    "        \"\"\"\n",
    "        Save predictions to the specified directory.\n",
    "\n",
    "        Parameters:\n",
    "        - logits: torch.Tensor, the logits output from the model.\n",
    "        - batch_idx: int, the index of the current batch.\n",
    "        - output_dir: str, directory where predictions will be saved.\n",
    "        - file_prefix: str, prefix for the output file names (optional).\n",
    "        \"\"\"\n",
    "        filename = str(batch_idx) + '.npy'\n",
    "        if file_prefix:\n",
    "            filename = file_prefix + '_' + filename\n",
    "        pred_file_path = os.path.join(output_dir, filename)\n",
    "        np.save(pred_file_path, logits.cpu())\n",
    "\n",
    "    def predict(self, dataloader, device='cuda', output_dir=None, file_prefix=None):\n",
    "        \"\"\"\n",
    "        Make predictions on a dataset and optionally save the results.\n",
    "\n",
    "        Parameters:\n",
    "        - dataloader: DataLoader, the DataLoader providing the images.\n",
    "        - device: str, the device to use for computation. Default is 'cuda'.\n",
    "        - output_dir: str, directory where predictions will be saved.\n",
    "        - file_prefix: str, prefix for the output file names.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        if output_dir is None: \n",
    "            print('No output dir for predictions specified.')\n",
    "            return\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "                images, _ = batch\n",
    "                images = images.to(device)\n",
    "                logits = self.model(images)\n",
    "                self.save_preds(logits, batch_idx, output_dir, file_prefix)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configure the optimizer and learning rate scheduler.\n",
    "\n",
    "        Returns:\n",
    "        - dict: containing the optimizer and learning rate scheduler.\n",
    "        \"\"\"\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.8, patience=10, min_lr=1e-05)\n",
    "        return {'optimizer': self.optimizer, 'lr_scheduler': scheduler, 'monitor': 'valid_loss'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fafc3d04-3598-4bc9-973d-9a802a3e8f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set matmul precision\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# Select model\n",
    "model_name = 'unet'\n",
    "encoder = 'resnet34'  \n",
    "pretrained = 'imagenet'\n",
    "\n",
    "# Name of the model\n",
    "run_name = model_name + '_' + encoder\n",
    "if pretrained: run_name += '_' + pretrained\n",
    "version_name = f\"{datetime.now().strftime('%d%m-%H%M')}\"\n",
    "run_name += '_' + version_name\n",
    "\n",
    "classes = 1\n",
    "\n",
    "# Create the model\n",
    "model = smp.create_model(model_name,\n",
    "                         encoder_name = encoder,\n",
    "                         in_channels = 3,\n",
    "                         encoder_weights=pretrained,\n",
    "                         classes = classes).to(device)\n",
    "   \n",
    "# Assuming class 0 is the background and class 1 is the class of interest\n",
    "# alpha shows relative importance of background vs class\n",
    "criterion = FocalLoss(alpha=torch.tensor([0.05, 0.95]), gamma=2.0, logits=True)\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-03)#, weight_decay=1e-05)\n",
    "\n",
    "# Set up how to save trained model weights\n",
    "# checkpoint_dir can be changed\n",
    "checkpoint_dir = f'./models/checkpoints_{run_name}'\n",
    "checkpoint = ModelCheckpoint(dirpath = checkpoint_dir,\n",
    "                                   #filename='{epoch:02d}-{valid_IoU:.2f}',\n",
    "                                   filename='best_model', \n",
    "                                   save_top_k=1,\n",
    "                                   verbose = True, \n",
    "                                   monitor = 'valid_loss', \n",
    "                                   mode = 'min')\n",
    "# Set up early training stopping\n",
    "early_stopping = EarlyStopping(monitor='valid_loss', patience=10, mode='min')\n",
    "# How often update learning rate\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15d75594-f5bd-45a6-bf41-47018f296289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: tb_logs/unet_resnet34_imagenet_1807-1054\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                      | Params\n",
      "---------------------------------------------------------------\n",
      "0 | model            | Unet                      | 24.4 M\n",
      "1 | criterion        | FocalLoss                 | 0     \n",
      "2 | iou              | MeanIoU                   | 0     \n",
      "3 | precision        | MulticlassPrecision       | 0     \n",
      "4 | recall           | MulticlassRecall          | 0     \n",
      "5 | f1               | MulticlassF1Score         | 0     \n",
      "6 | dice             | GeneralizedDiceScore      | 0     \n",
      "7 | confusion_matrix | MulticlassConfusionMatrix | 0     \n",
      "8 | auroc            | BinaryAUROC               | 0     \n",
      "---------------------------------------------------------------\n",
      "24.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.4 M    Total params\n",
      "97.745    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/cuda/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536700bed6554ed3b948bd9b2903c627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 189: 'valid_loss' reached 0.00013 (best 0.00013), saving model to '/home/ubuntu/SSP/echosounder-segmentation/models/checkpoints_unet_resnet34_imagenet_1807-1054/best_model.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 378: 'valid_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 567: 'valid_loss' reached 0.00009 (best 0.00009), saving model to '/home/ubuntu/SSP/echosounder-segmentation/models/checkpoints_unet_resnet34_imagenet_1807-1054/best_model.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 756: 'valid_loss' reached 0.00009 (best 0.00009), saving model to '/home/ubuntu/SSP/echosounder-segmentation/models/checkpoints_unet_resnet34_imagenet_1807-1054/best_model.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 945: 'valid_loss' reached 0.00008 (best 0.00008), saving model to '/home/ubuntu/SSP/echosounder-segmentation/models/checkpoints_unet_resnet34_imagenet_1807-1054/best_model.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 1134: 'valid_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 1323: 'valid_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 1512: 'valid_loss' reached 0.00008 (best 0.00008), saving model to '/home/ubuntu/SSP/echosounder-segmentation/models/checkpoints_unet_resnet34_imagenet_1807-1054/best_model.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 1701: 'valid_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 1890: 'valid_loss' reached 0.00006 (best 0.00006), saving model to '/home/ubuntu/SSP/echosounder-segmentation/models/checkpoints_unet_resnet34_imagenet_1807-1054/best_model.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 2079: 'valid_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 2268: 'valid_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 2457: 'valid_loss' reached 0.00006 (best 0.00006), saving model to '/home/ubuntu/SSP/echosounder-segmentation/models/checkpoints_unet_resnet34_imagenet_1807-1054/best_model.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 2646: 'valid_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 2835: 'valid_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 3024: 'valid_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 3213: 'valid_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 3402: 'valid_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 3591: 'valid_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 3780: 'valid_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 3969: 'valid_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 4158: 'valid_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 4347: 'valid_loss' was not in top 1\n",
      "FIT Profiler Report\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  Action                                                                                                                                                                 \t|  Mean duration (s)\t|  Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  Total                                                                                                                                                                  \t|  -              \t|  231774         \t|  1.3825e+04     \t|  100 %          \t|\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  run_training_epoch                                                                                                                                                     \t|  600.58         \t|  23             \t|  1.3813e+04     \t|  99.916         \t|\n",
      "|  [_TrainingEpochLoop].train_dataloader_next                                                                                                                             \t|  2.7295         \t|  4347           \t|  1.1865e+04     \t|  85.825         \t|\n",
      "|  [_EvaluationLoop].val_next                                                                                                                                             \t|  1.3052         \t|  1224           \t|  1597.5         \t|  11.555         \t|\n",
      "|  run_training_batch                                                                                                                                                     \t|  0.072451       \t|  4347           \t|  314.94         \t|  2.2781         \t|\n",
      "|  [LightningModule]SegModel.optimizer_step                                                                                                                               \t|  0.072322       \t|  4347           \t|  314.38         \t|  2.274          \t|\n",
      "|  [Strategy]SingleDeviceStrategy.training_step                                                                                                                           \t|  0.025555       \t|  4347           \t|  111.09         \t|  0.80353        \t|\n",
      "|  [Strategy]SingleDeviceStrategy.backward                                                                                                                                \t|  0.006714       \t|  4347           \t|  29.186         \t|  0.21111        \t|\n",
      "|  [Strategy]SingleDeviceStrategy.validation_step                                                                                                                         \t|  0.02153        \t|  1224           \t|  26.352         \t|  0.19061        \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'valid_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_epoch_end       \t|  0.19089        \t|  23             \t|  4.3904         \t|  0.031758       \t|\n",
      "|  [LightningModule]SegModel.configure_gradient_clipping                                                                                                                  \t|  0.00074613     \t|  4347           \t|  3.2434         \t|  0.023461       \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_batch_end                                                                                                                           \t|  0.00053967     \t|  4347           \t|  2.3459         \t|  0.016969       \t|\n",
      "|  [Strategy]SingleDeviceStrategy.batch_to_device                                                                                                                         \t|  0.00040641     \t|  5571           \t|  2.2641         \t|  0.016377       \t|\n",
      "|  [LightningModule]SegModel.transfer_batch_to_device                                                                                                                     \t|  0.00037093     \t|  5571           \t|  2.0665         \t|  0.014947       \t|\n",
      "|  [LightningModule]SegModel.optimizer_zero_grad                                                                                                                          \t|  0.00031524     \t|  4347           \t|  1.3703         \t|  0.009912       \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_batch_end                                                                                                                      \t|  0.0004192      \t|  1224           \t|  0.5131         \t|  0.0037114      \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_start                                                                                                                          \t|  0.0072885      \t|  24             \t|  0.17492        \t|  0.0012653      \t|\n",
      "|  [LightningModule]SegModel.on_validation_model_zero_grad                                                                                                                \t|  0.003733       \t|  23             \t|  0.08586        \t|  0.00062105     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'valid_loss', 'mode': 'min'}.on_train_epoch_end                                                                                     \t|  0.0031298      \t|  23             \t|  0.071985       \t|  0.00052069     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'valid_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.setup                    \t|  0.065683       \t|  1              \t|  0.065683       \t|  0.00047511     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'valid_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_batch_end       \t|  1.1913e-05     \t|  4347           \t|  0.051785       \t|  0.00037458     \t|\n",
      "|  [Callback]LearningRateMonitor.on_train_batch_start                                                                                                                     \t|  5.7369e-06     \t|  4347           \t|  0.024938       \t|  0.00018039     \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_end                                                                                                                            \t|  0.0010389      \t|  24             \t|  0.024933       \t|  0.00018035     \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_batch_start                                                                                                                    \t|  1.8633e-05     \t|  1224           \t|  0.022807       \t|  0.00016497     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'valid_loss', 'mode': 'min'}.on_after_backward                                                                                      \t|  3.5761e-06     \t|  4347           \t|  0.015545       \t|  0.00011244     \t|\n",
      "|  [LightningModule]SegModel.on_validation_model_eval                                                                                                                     \t|  0.0005723      \t|  24             \t|  0.013735       \t|  9.9351e-05     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'valid_loss', 'mode': 'min'}.on_before_optimizer_step                                                                               \t|  2.795e-06      \t|  4347           \t|  0.01215        \t|  8.7885e-05     \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_epoch_start                                                                                                                         \t|  0.00049787     \t|  23             \t|  0.011451       \t|  8.2829e-05     \t|\n",
      "|  [LightningModule]SegModel.lr_scheduler_step                                                                                                                            \t|  0.00045285     \t|  23             \t|  0.010415       \t|  7.5338e-05     \t|\n",
      "|  [LightningModule]SegModel.on_before_batch_transfer                                                                                                                     \t|  1.5729e-06     \t|  5571           \t|  0.0087626      \t|  6.3383e-05     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'valid_loss', 'mode': 'min'}.on_train_batch_end                                                                                     \t|  2.0095e-06     \t|  4347           \t|  0.0087354      \t|  6.3186e-05     \t|\n",
      "|  [Callback]ModelSummary.on_train_batch_end                                                                                                                              \t|  1.9502e-06     \t|  4347           \t|  0.0084777      \t|  6.1322e-05     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'valid_loss', 'mode': 'min'}.on_train_batch_start                                                                                   \t|  1.8523e-06     \t|  4347           \t|  0.0080519      \t|  5.8242e-05     \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_epoch_end                                                                                                                           \t|  0.00033917     \t|  23             \t|  0.0078008      \t|  5.6426e-05     \t|\n",
      "|  [Callback]LearningRateMonitor.on_after_backward                                                                                                                        \t|  1.72e-06       \t|  4347           \t|  0.0074769      \t|  5.4083e-05     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'valid_loss', 'mode': 'min'}.on_before_zero_grad                                                                                    \t|  1.6541e-06     \t|  4347           \t|  0.0071902      \t|  5.2009e-05     \t|\n",
      "|  [Callback]LearningRateMonitor.on_before_optimizer_step                                                                                                                 \t|  1.5303e-06     \t|  4347           \t|  0.0066521      \t|  4.8117e-05     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'valid_loss', 'mode': 'min'}.on_before_backward                                                                                     \t|  1.5113e-06     \t|  4347           \t|  0.0065694      \t|  4.7519e-05     \t|\n",
      "|  [LightningModule]SegModel.on_after_batch_transfer                                                                                                                      \t|  1.1677e-06     \t|  5571           \t|  0.0065055      \t|  4.7057e-05     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'valid_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_after_backward        \t|  1.3738e-06     \t|  4347           \t|  0.0059719      \t|  4.3196e-05     \t|\n",
      "|  [LightningModule]SegModel.on_after_backward                                                                                                                            \t|  1.2333e-06     \t|  4347           \t|  0.0053612      \t|  3.8779e-05     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'valid_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_before_optimizer_step \t|  1.1677e-06     \t|  4347           \t|  0.005076       \t|  3.6717e-05     \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_start                                                                                                                               \t|  0.005064       \t|  1              \t|  0.005064       \t|  3.6629e-05     \t|\n",
      "|  [Callback]TQDMProgressBar.on_after_backward                                                                                                                            \t|  1.1518e-06     \t|  4347           \t|  0.005007       \t|  3.6217e-05     \t|\n",
      "|  [Callback]TQDMProgressBar.on_before_optimizer_step                                                                                                                     \t|  1.1066e-06     \t|  4347           \t|  0.0048104      \t|  3.4795e-05     \t|\n",
      "|  [Callback]ModelSummary.on_after_backward                                                                                                                               \t|  1.0782e-06     \t|  4347           \t|  0.0046869      \t|  3.3902e-05     \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_batch_start                                                                                                                         \t|  1.0757e-06     \t|  4347           \t|  0.004676       \t|  3.3823e-05     \t|\n",
      "|  [Callback]TQDMProgressBar.on_sanity_check_start                                                                                                                        \t|  0.0045512      \t|  1              \t|  0.0045512      \t|  3.292e-05      \t|\n",
      "|  [LightningModule]SegModel.on_before_optimizer_step                                                                                                                     \t|  1.0396e-06     \t|  4347           \t|  0.004519       \t|  3.2687e-05     \t|\n",
      "|  [Callback]ModelSummary.on_before_optimizer_step                                                                                                                        \t|  9.6745e-07     \t|  4347           \t|  0.0042055      \t|  3.042e-05      \t|\n",
      "|  [Callback]LearningRateMonitor.on_before_zero_grad                                                                                                                      \t|  8.6616e-07     \t|  4347           \t|  0.0037652      \t|  2.7235e-05     \t|\n",
      "|  [LightningModule]SegModel.on_train_batch_start                                                                                                                         \t|  8.6162e-07     \t|  4347           \t|  0.0037454      \t|  2.7092e-05     \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_train_batch_start                                                                                                                    \t|  8.3847e-07     \t|  4347           \t|  0.0036448      \t|  2.6364e-05     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'valid_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_batch_start     \t|  8.2246e-07     \t|  4347           \t|  0.0035752      \t|  2.5861e-05     \t|\n",
      "|  [Callback]ModelSummary.on_train_batch_start                                                                                                                            \t|  8.1026e-07     \t|  4347           \t|  0.0035222      \t|  2.5477e-05     \t|\n",
      "|  [LightningModule]SegModel.on_before_zero_grad                                                                                                                          \t|  8.0498e-07     \t|  4347           \t|  0.0034992      \t|  2.5311e-05     \t|\n",
      "|  [Callback]LearningRateMonitor.on_train_batch_end                                                                                                                       \t|  7.727e-07      \t|  4347           \t|  0.0033589      \t|  2.4296e-05     \t|\n",
      "|  [Callback]LearningRateMonitor.on_train_epoch_start                                                                                                                     \t|  0.00014222     \t|  23             \t|  0.003271       \t|  2.3661e-05     \t|\n",
      "|  [LightningModule]SegModel.on_train_batch_end                                                                                                                           \t|  7.3274e-07     \t|  4347           \t|  0.0031852      \t|  2.304e-05      \t|\n",
      "|  [Callback]LearningRateMonitor.on_before_backward                                                                                                                       \t|  6.4218e-07     \t|  4347           \t|  0.0027916      \t|  2.0192e-05     \t|\n",
      "|  [LightningModule]SegModel.on_before_backward                                                                                                                           \t|  6.2222e-07     \t|  4347           \t|  0.0027048      \t|  1.9564e-05     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'valid_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_before_backward       \t|  6.0826e-07     \t|  4347           \t|  0.0026441      \t|  1.9126e-05     \t|\n",
      "|  [Callback]TQDMProgressBar.on_before_zero_grad                                                                                                                          \t|  6.0013e-07     \t|  4347           \t|  0.0026088      \t|  1.887e-05      \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'valid_loss', 'mode': 'min'}.on_validation_batch_start                                                                              \t|  2.1273e-06     \t|  1224           \t|  0.0026038      \t|  1.8834e-05     \t|\n",
      "|  [Callback]ModelSummary.on_fit_start                                                                                                                                    \t|  0.0025956      \t|  1              \t|  0.0025956      \t|  1.8775e-05     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'valid_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_before_zero_grad      \t|  5.3211e-07     \t|  4347           \t|  0.0023131      \t|  1.6731e-05     \t|\n",
      "|  [Callback]ModelSummary.on_before_zero_grad                                                                                                                             \t|  5.134e-07      \t|  4347           \t|  0.0022317      \t|  1.6143e-05     \t|\n",
      "|  [Callback]TQDMProgressBar.on_before_backward                                                                                                                           \t|  5.0306e-07     \t|  4347           \t|  0.0021868      \t|  1.5818e-05     \t|\n",
      "|  [Callback]ModelSummary.on_before_backward                                                                                                                              \t|  4.6239e-07     \t|  4347           \t|  0.00201        \t|  1.4539e-05     \t|\n",
      "|  [Callback]ModelSummary.on_validation_batch_end                                                                                                                         \t|  1.6309e-06     \t|  1224           \t|  0.0019962      \t|  1.4439e-05     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'valid_loss', 'mode': 'min'}.on_validation_batch_end                                                                                \t|  1.5147e-06     \t|  1224           \t|  0.001854       \t|  1.3411e-05     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'valid_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_end        \t|  7.5733e-05     \t|  24             \t|  0.0018176      \t|  1.3147e-05     \t|\n",
      "|  [LightningModule]SegModel.on_validation_batch_start                                                                                                                    \t|  1.3656e-06     \t|  1224           \t|  0.0016714      \t|  1.209e-05      \t|\n",
      "|  [Callback]LearningRateMonitor.on_validation_batch_start                                                                                                                \t|  1.0603e-06     \t|  1224           \t|  0.0012978      \t|  9.3871e-06     \t|\n",
      "|  [Callback]ModelSummary.on_validation_batch_start                                                                                                                       \t|  1.0333e-06     \t|  1224           \t|  0.0012648      \t|  9.1487e-06     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'valid_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_batch_start\t|  1.0101e-06     \t|  1224           \t|  0.0012364      \t|  8.9432e-06     \t|\n",
      "|  [LightningModule]SegModel.on_validation_batch_end                                                                                                                      \t|  8.0942e-07     \t|  1224           \t|  0.00099073     \t|  7.1663e-06     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'valid_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_batch_end  \t|  7.1124e-07     \t|  1224           \t|  0.00087055     \t|  6.297e-06      \t|\n",
      "|  [Callback]LearningRateMonitor.on_validation_batch_end                                                                                                                  \t|  6.8559e-07     \t|  1224           \t|  0.00083916     \t|  6.0699e-06     \t|\n",
      "|  [Callback]TQDMProgressBar.on_train_end                                                                                                                                 \t|  0.00016829     \t|  1              \t|  0.00016829     \t|  1.2173e-06     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'valid_loss', 'mode': 'min'}.on_validation_end                                                                                      \t|  6.7144e-06     \t|  24             \t|  0.00016115     \t|  1.1656e-06     \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_validation_start                                                                                                                     \t|  4.2495e-06     \t|  24             \t|  0.00010199     \t|  7.3772e-07     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'valid_loss', 'mode': 'min'}.on_validation_epoch_start                                                                              \t|  3.9195e-06     \t|  24             \t|  9.4068e-05     \t|  6.8043e-07     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'valid_loss', 'mode': 'min'}.on_validation_start                                                                                    \t|  3.6113e-06     \t|  24             \t|  8.6672e-05     \t|  6.2693e-07     \t|\n",
      "|  [Callback]ModelSummary.on_validation_start                                                                                                                             \t|  3.4407e-06     \t|  24             \t|  8.2576e-05     \t|  5.973e-07      \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_validation_end                                                                                                                       \t|  3.3158e-06     \t|  24             \t|  7.9579e-05     \t|  5.7562e-07     \t|\n",
      "|  [LightningModule]SegModel.on_validation_start                                                                                                                          \t|  2.5945e-06     \t|  24             \t|  6.2267e-05     \t|  4.504e-07      \t|\n",
      "|  [Callback]LearningRateMonitor.on_train_start                                                                                                                           \t|  5.4362e-05     \t|  1              \t|  5.4362e-05     \t|  3.9322e-07     \t|\n",
      "|  [Callback]ModelSummary.on_validation_end                                                                                                                               \t|  1.974e-06      \t|  24             \t|  4.7377e-05     \t|  3.4269e-07     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'valid_loss', 'mode': 'min'}.on_train_epoch_start                                                                                   \t|  1.7289e-06     \t|  23             \t|  3.9764e-05     \t|  2.8763e-07     \t|\n",
      "|  [Callback]LearningRateMonitor.on_validation_end                                                                                                                        \t|  1.5017e-06     \t|  24             \t|  3.604e-05      \t|  2.6069e-07     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'valid_loss', 'mode': 'min'}.on_validation_epoch_end                                                                                \t|  1.4849e-06     \t|  24             \t|  3.5637e-05     \t|  2.5777e-07     \t|\n",
      "|  [Callback]ModelSummary.on_train_epoch_end                                                                                                                              \t|  1.4644e-06     \t|  23             \t|  3.3682e-05     \t|  2.4363e-07     \t|\n",
      "|  [Callback]ModelSummary.on_train_epoch_start                                                                                                                            \t|  1.3789e-06     \t|  23             \t|  3.1715e-05     \t|  2.2941e-07     \t|\n",
      "|  [Callback]LearningRateMonitor.on_train_epoch_end                                                                                                                       \t|  1.368e-06      \t|  23             \t|  3.1463e-05     \t|  2.2758e-07     \t|\n",
      "|  [Callback]LearningRateMonitor.on_validation_epoch_end                                                                                                                  \t|  9.2341e-07     \t|  24             \t|  2.2162e-05     \t|  1.603e-07      \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'valid_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_start      \t|  8.8737e-07     \t|  24             \t|  2.1297e-05     \t|  1.5405e-07     \t|\n",
      "|  [LightningModule]SegModel.on_validation_end                                                                                                                            \t|  8.2367e-07     \t|  24             \t|  1.9768e-05     \t|  1.4299e-07     \t|\n",
      "|  [Callback]LearningRateMonitor.on_validation_start                                                                                                                      \t|  7.317e-07      \t|  24             \t|  1.7561e-05     \t|  1.2702e-07     \t|\n",
      "|  [LightningModule]SegModel.on_validation_epoch_end                                                                                                                      \t|  7.005e-07      \t|  24             \t|  1.6812e-05     \t|  1.2161e-07     \t|\n",
      "|  [LightningModule]SegModel.configure_optimizers                                                                                                                         \t|  1.618e-05      \t|  1              \t|  1.618e-05      \t|  1.1704e-07     \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_epoch_end                                                                                                                      \t|  6.7342e-07     \t|  24             \t|  1.6162e-05     \t|  1.1691e-07     \t|\n",
      "|  [LightningModule]SegModel.on_validation_epoch_start                                                                                                                    \t|  6.3139e-07     \t|  24             \t|  1.5153e-05     \t|  1.0961e-07     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'valid_loss', 'mode': 'min'}.on_train_start                                                                                         \t|  1.4767e-05     \t|  1              \t|  1.4767e-05     \t|  1.0681e-07     \t|\n",
      "|  [LightningModule]SegModel.on_train_epoch_start                                                                                                                         \t|  6.3078e-07     \t|  23             \t|  1.4508e-05     \t|  1.0494e-07     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'valid_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_epoch_start     \t|  6.3035e-07     \t|  23             \t|  1.4498e-05     \t|  1.0487e-07     \t|\n",
      "|  [LightningModule]SegModel.on_train_epoch_end                                                                                                                           \t|  6.1343e-07     \t|  23             \t|  1.4109e-05     \t|  1.0205e-07     \t|\n",
      "|  [Callback]LearningRateMonitor.on_validation_epoch_start                                                                                                                \t|  5.7651e-07     \t|  24             \t|  1.3836e-05     \t|  1.0008e-07     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'valid_loss', 'mode': 'min'}.on_save_checkpoint                                                                                     \t|  1.9677e-06     \t|  7              \t|  1.3774e-05     \t|  9.9631e-08     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'valid_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_epoch_start\t|  5.6776e-07     \t|  24             \t|  1.3626e-05     \t|  9.8562e-08     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'valid_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_epoch_end  \t|  5.5438e-07     \t|  24             \t|  1.3305e-05     \t|  9.624e-08      \t|\n",
      "|  [Callback]ModelSummary.on_validation_epoch_end                                                                                                                         \t|  5.4445e-07     \t|  24             \t|  1.3067e-05     \t|  9.4517e-08     \t|\n",
      "|  [Callback]TQDMProgressBar.on_validation_epoch_start                                                                                                                    \t|  4.8971e-07     \t|  24             \t|  1.1753e-05     \t|  8.5013e-08     \t|\n",
      "|  [Callback]ModelSummary.on_validation_epoch_start                                                                                                                       \t|  4.5587e-07     \t|  24             \t|  1.0941e-05     \t|  7.9139e-08     \t|\n",
      "|  [Callback]TQDMProgressBar.on_sanity_check_end                                                                                                                          \t|  5.68e-06       \t|  1              \t|  5.68e-06       \t|  4.1085e-08     \t|\n",
      "|  [Callback]LearningRateMonitor.on_save_checkpoint                                                                                                                       \t|  7.6871e-07     \t|  7              \t|  5.3809e-06     \t|  3.8922e-08     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'valid_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_fit_start             \t|  5.239e-06      \t|  1              \t|  5.239e-06      \t|  3.7895e-08     \t|\n",
      "|  [Callback]TQDMProgressBar.on_save_checkpoint                                                                                                                           \t|  7.3543e-07     \t|  7              \t|  5.148e-06      \t|  3.7237e-08     \t|\n",
      "|  [Callback]ModelSummary.on_save_checkpoint                                                                                                                              \t|  6.7713e-07     \t|  7              \t|  4.7399e-06     \t|  3.4285e-08     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'valid_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_save_checkpoint       \t|  6.0401e-07     \t|  7              \t|  4.2281e-06     \t|  3.0583e-08     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'valid_loss', 'mode': 'min'}.setup                                                                                                  \t|  3.807e-06      \t|  1              \t|  3.807e-06      \t|  2.7537e-08     \t|\n",
      "|  [LightningModule]SegModel.on_save_checkpoint                                                                                                                           \t|  4.537e-07      \t|  7              \t|  3.1759e-06     \t|  2.2973e-08     \t|\n",
      "|  [LightningModule]SegModel.on_train_start                                                                                                                               \t|  2.866e-06      \t|  1              \t|  2.866e-06      \t|  2.073e-08      \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_train_start                                                                                                                          \t|  2.685e-06      \t|  1              \t|  2.685e-06      \t|  1.9421e-08     \t|\n",
      "|  [Callback]TQDMProgressBar.setup                                                                                                                                        \t|  2.575e-06      \t|  1              \t|  2.575e-06      \t|  1.8626e-08     \t|\n",
      "|  [Callback]ModelSummary.on_train_start                                                                                                                                  \t|  2.054e-06      \t|  1              \t|  2.054e-06      \t|  1.4857e-08     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'valid_loss', 'mode': 'min'}.on_fit_end                                                                                             \t|  1.894e-06      \t|  1              \t|  1.894e-06      \t|  1.37e-08       \t|\n",
      "|  [Callback]ModelSummary.on_sanity_check_start                                                                                                                           \t|  1.854e-06      \t|  1              \t|  1.854e-06      \t|  1.341e-08      \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'valid_loss', 'mode': 'min'}.on_fit_start                                                                                           \t|  1.804e-06      \t|  1              \t|  1.804e-06      \t|  1.3049e-08     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'valid_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_start           \t|  1.793e-06      \t|  1              \t|  1.793e-06      \t|  1.297e-08      \t|\n",
      "|  [Callback]LearningRateMonitor.setup                                                                                                                                    \t|  1.633e-06      \t|  1              \t|  1.633e-06      \t|  1.1812e-08     \t|\n",
      "|  [LightningModule]SegModel.setup                                                                                                                                        \t|  1.613e-06      \t|  1              \t|  1.613e-06      \t|  1.1667e-08     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'valid_loss', 'mode': 'min'}.on_sanity_check_start                                                                                  \t|  1.573e-06      \t|  1              \t|  1.573e-06      \t|  1.1378e-08     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'valid_loss', 'mode': 'min'}.on_sanity_check_end                                                                                    \t|  1.443e-06      \t|  1              \t|  1.443e-06      \t|  1.0438e-08     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'valid_loss', 'mode': 'min'}.on_train_end                                                                                           \t|  1.442e-06      \t|  1              \t|  1.442e-06      \t|  1.0431e-08     \t|\n",
      "|  [LightningModule]SegModel.configure_callbacks                                                                                                                          \t|  1.362e-06      \t|  1              \t|  1.362e-06      \t|  9.8518e-09     \t|\n",
      "|  [Callback]ModelSummary.on_train_end                                                                                                                                    \t|  1.352e-06      \t|  1              \t|  1.352e-06      \t|  9.7794e-09     \t|\n",
      "|  [Callback]EarlyStopping{'monitor': 'valid_loss', 'mode': 'min'}.teardown                                                                                               \t|  1.162e-06      \t|  1              \t|  1.162e-06      \t|  8.4051e-09     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'valid_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_sanity_check_start    \t|  9.9203e-07     \t|  1              \t|  9.9203e-07     \t|  7.1757e-09     \t|\n",
      "|  [LightningModule]SegModel.prepare_data                                                                                                                                 \t|  9.12e-07       \t|  1              \t|  9.12e-07       \t|  6.5968e-09     \t|\n",
      "|  [Callback]LearningRateMonitor.on_fit_start                                                                                                                             \t|  8.5199e-07     \t|  1              \t|  8.5199e-07     \t|  6.1627e-09     \t|\n",
      "|  [Callback]ModelSummary.setup                                                                                                                                           \t|  8.51e-07       \t|  1              \t|  8.51e-07       \t|  6.1555e-09     \t|\n",
      "|  [LightningModule]SegModel.teardown                                                                                                                                     \t|  8.0105e-07     \t|  1              \t|  8.0105e-07     \t|  5.7943e-09     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'valid_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_end             \t|  7.7096e-07     \t|  1              \t|  7.7096e-07     \t|  5.5766e-09     \t|\n",
      "|  [Callback]LearningRateMonitor.teardown                                                                                                                                 \t|  7.4098e-07     \t|  1              \t|  7.4098e-07     \t|  5.3598e-09     \t|\n",
      "|  [Callback]LearningRateMonitor.on_sanity_check_end                                                                                                                      \t|  7.3097e-07     \t|  1              \t|  7.3097e-07     \t|  5.2874e-09     \t|\n",
      "|  [Callback]TQDMProgressBar.on_fit_end                                                                                                                                   \t|  7.12e-07       \t|  1              \t|  7.12e-07       \t|  5.1501e-09     \t|\n",
      "|  [Callback]LearningRateMonitor.on_fit_end                                                                                                                               \t|  7.1101e-07     \t|  1              \t|  7.1101e-07     \t|  5.1429e-09     \t|\n",
      "|  [Callback]LearningRateMonitor.on_sanity_check_start                                                                                                                    \t|  6.7102e-07     \t|  1              \t|  6.7102e-07     \t|  4.8537e-09     \t|\n",
      "|  [Strategy]SingleDeviceStrategy.on_train_end                                                                                                                            \t|  6.5099e-07     \t|  1              \t|  6.5099e-07     \t|  4.7089e-09     \t|\n",
      "|  [LightningModule]SegModel.on_fit_start                                                                                                                                 \t|  6.4104e-07     \t|  1              \t|  6.4104e-07     \t|  4.6369e-09     \t|\n",
      "|  [Callback]LearningRateMonitor.on_train_end                                                                                                                             \t|  6.2102e-07     \t|  1              \t|  6.2102e-07     \t|  4.492e-09      \t|\n",
      "|  [Callback]ModelSummary.teardown                                                                                                                                        \t|  6.1101e-07     \t|  1              \t|  6.1101e-07     \t|  4.4196e-09     \t|\n",
      "|  [Callback]TQDMProgressBar.on_fit_start                                                                                                                                 \t|  6.1095e-07     \t|  1              \t|  6.1095e-07     \t|  4.4192e-09     \t|\n",
      "|  [LightningModule]SegModel.on_train_end                                                                                                                                 \t|  6.0099e-07     \t|  1              \t|  6.0099e-07     \t|  4.3472e-09     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'valid_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.teardown                 \t|  5.8103e-07     \t|  1              \t|  5.8103e-07     \t|  4.2028e-09     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'valid_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_fit_end               \t|  5.8097e-07     \t|  1              \t|  5.8097e-07     \t|  4.2023e-09     \t|\n",
      "|  [LightningModule]SegModel.on_fit_end                                                                                                                                   \t|  5.6101e-07     \t|  1              \t|  5.6101e-07     \t|  4.0579e-09     \t|\n",
      "|  [Callback]TQDMProgressBar.teardown                                                                                                                                     \t|  5.6101e-07     \t|  1              \t|  5.6101e-07     \t|  4.0579e-09     \t|\n",
      "|  [Callback]ModelSummary.on_sanity_check_end                                                                                                                             \t|  5.4098e-07     \t|  1              \t|  5.4098e-07     \t|  3.9131e-09     \t|\n",
      "|  [Callback]ModelCheckpoint{'monitor': 'valid_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_sanity_check_end      \t|  5.2102e-07     \t|  1              \t|  5.2102e-07     \t|  3.7687e-09     \t|\n",
      "|  [Callback]ModelSummary.on_fit_end                                                                                                                                      \t|  4.4098e-07     \t|  1              \t|  4.4098e-07     \t|  3.1898e-09     \t|\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pl_model = SegModel(model, criterion, optimizer)\n",
    "\n",
    "# Select profiler\n",
    "profiler = SimpleProfiler()\n",
    "\n",
    "# Initialize the logger\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=\"tb_logs\",\n",
    "    name=f'{run_name}'\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = pl.Trainer(\n",
    "    profiler=profiler,\n",
    "    num_sanity_val_steps=5,\n",
    "    logger=logger,\n",
    "    gradient_clip_val=0.5,\n",
    "    precision='16-mixed',\n",
    "    accelerator='gpu',\n",
    "    max_epochs=100,\n",
    "    callbacks=[checkpoint, early_stopping, lr_monitor],\n",
    "    #val_check_interval=0.1  # Validate more frequently\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(pl_model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2056a3f-a713-4c64-823d-3cb84b347c19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
